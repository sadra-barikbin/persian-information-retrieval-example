{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "with open('docs.txt') as fp:\n",
    "    docs = [d.strip() for d in fp.readlines()]\n",
    "docs_r = {k:i for i, k in enumerate(docs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../dataset/IR_dataset/2048.txt',\n",
       " '../dataset/IR_dataset/2404.txt',\n",
       " '../dataset/IR_dataset/661.txt',\n",
       " '../dataset/IR_dataset/1252.txt',\n",
       " '../dataset/IR_dataset/726.txt',\n",
       " '../dataset/IR_dataset/3029.txt',\n",
       " '../dataset/IR_dataset/329.txt',\n",
       " '../dataset/IR_dataset/1481.txt',\n",
       " '../dataset/IR_dataset/1511.txt',\n",
       " '../dataset/IR_dataset/1127.txt',\n",
       " '../dataset/IR_dataset/2447.txt',\n",
       " '../dataset/IR_dataset/2907.txt',\n",
       " '../dataset/IR_dataset/630.txt',\n",
       " '../dataset/IR_dataset/3008.txt',\n",
       " '../dataset/IR_dataset/494.txt',\n",
       " '../dataset/IR_dataset/321.txt',\n",
       " '../dataset/IR_dataset/40.txt',\n",
       " '../dataset/IR_dataset/3131.txt',\n",
       " '../dataset/IR_dataset/2192.txt',\n",
       " '../dataset/IR_dataset/2167.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dev Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../dataset/evaluation_IR.yml', 'r') as f:\n",
    "    dataset = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dev = {k:v for k, v in list(dataset.items())[-30:]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['نظریه اصل موضوعی مجموعه\\u200cها  سازگاری و عدم وابستگی در ZFC',\n",
       " 'نظریه نسبیت  نسبیت عام',\n",
       " 'نلسون ماندلا  آغاز',\n",
       " 'نهنگ قاتل  هوش',\n",
       " 'نوا (دستگاه موسیقی)  گوشه\\u200cها',\n",
       " 'نیروهای محور  اتحاد دانوب، اختلاف بر سر اتریش',\n",
       " 'هاینریش هیملر  استحکام قدرت',\n",
       " 'هاینریش هیملر  رابطه با هیتلر',\n",
       " 'هسته لینوکس',\n",
       " 'هسته لینوکس  درگیری\\u200cهای جامعه توسعه',\n",
       " 'هسته لینوکس  مدل توسعه',\n",
       " 'هم\\u200cارزی جرم و انرژی  کاربست\\u200cپذیری فرمول',\n",
       " 'هندسه جبری',\n",
       " 'هوش مصنوعی  تاریخچه',\n",
       " 'واپاشی هسته\\u200cای  پایداری و ناپایداری ایزوتوپ\\u200cها',\n",
       " 'وشمگیر  وضعیت سیاسی-اجتماعی قرن چهارم هجری',\n",
       " 'ولایت قندهار  تمدن مندیگک',\n",
       " 'ولفگانگ آمادئوس موتسارت  موتسارت در وین',\n",
       " 'ونکوور  سیستم حمل و نقل شهری',\n",
       " 'ونکوور  معماری',\n",
       " 'پروین اعتصامی',\n",
       " 'پرچم ایران  پیش از پادشاهی پهلوی\\u200cها',\n",
       " 'پیمان کیوتو  اتحادیه اروپا',\n",
       " 'چرخه آب  توصیف',\n",
       " 'چنگیز خان  کودکی',\n",
       " 'چهاردهمین دالایی لاما  اوان زندگی و سابقه',\n",
       " 'کارل مارکس  اقتصاد، تاریخ و جامعه',\n",
       " 'گرجستان  تاریخ',\n",
       " 'یانی  تأثیرپذیری\\u200cهای موسیقایی',\n",
       " 'یونسکو  فعالیت\\u200cها']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dataset_dev.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'relevant': [388],\n",
       " 'similar_high': [389, 390, 391, 392, 393, 394],\n",
       " 'similar_low': [404, 405, 406, 407, 408, 409, 410, 411, 412, 413],\n",
       " 'similar_med': [395, 364, 396, 397, 398, 399, 400, 401, 402, 403]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dev['گرجستان  تاریخ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "queries = pd.Series(list(dataset.keys()))\n",
    "qrels = [{'query_id':q, 'doc_id':str(d),\n",
    "          'relevance':3} for idx,q in queries.to_dict().items() for d in dataset[q]['similar_high']]\n",
    "qrels.extend([{'query_id':q, 'doc_id':str(d),\n",
    "          'relevance':2} for idx,q in queries.to_dict().items() for d in dataset[q]['similar_med']])\n",
    "qrels.extend([{'query_id':q, 'doc_id':str(d),\n",
    "          'relevance':1} for idx,q in queries.to_dict().items() for d in dataset[q]['similar_low']])\n",
    "qrels.extend([{'query_id':q, 'doc_id':str(dataset[q]['relevant'][0]),\n",
    "          'relevance':4} for idx,q in queries.to_dict().items()])\n",
    "qrels = pd.DataFrame(qrels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>نظریه اصل موضوعی مجموعه‌ها  سازگاری و عدم وابس...</td>\n",
       "      <td>2713</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>نظریه اصل موضوعی مجموعه‌ها  سازگاری و عدم وابس...</td>\n",
       "      <td>2714</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>نظریه اصل موضوعی مجموعه‌ها  سازگاری و عدم وابس...</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>نظریه نسبیت  نسبیت عام</td>\n",
       "      <td>1617</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>نظریه نسبیت  نسبیت عام</td>\n",
       "      <td>1619</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>چهاردهمین دالایی لاما  اوان زندگی و سابقه</td>\n",
       "      <td>2968</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>کارل مارکس  اقتصاد، تاریخ و جامعه</td>\n",
       "      <td>365</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>گرجستان  تاریخ</td>\n",
       "      <td>388</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>یانی  تأثیرپذیری‌های موسیقایی</td>\n",
       "      <td>1748</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>یونسکو  فعالیت‌ها</td>\n",
       "      <td>1505</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>732 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              query_id doc_id  relevance\n",
       "0    نظریه اصل موضوعی مجموعه‌ها  سازگاری و عدم وابس...   2713          3\n",
       "1    نظریه اصل موضوعی مجموعه‌ها  سازگاری و عدم وابس...   2714          3\n",
       "2    نظریه اصل موضوعی مجموعه‌ها  سازگاری و عدم وابس...    661          3\n",
       "3                               نظریه نسبیت  نسبیت عام   1617          3\n",
       "4                               نظریه نسبیت  نسبیت عام   1619          3\n",
       "..                                                 ...    ...        ...\n",
       "727          چهاردهمین دالایی لاما  اوان زندگی و سابقه   2968          4\n",
       "728                  کارل مارکس  اقتصاد، تاریخ و جامعه    365          4\n",
       "729                                     گرجستان  تاریخ    388          4\n",
       "730                      یانی  تأثیرپذیری‌های موسیقایی   1748          4\n",
       "731                                  یونسکو  فعالیت‌ها   1505          4\n",
       "\n",
       "[732 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qrels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open('pos/words.txt') as fp:\n",
    "    words_all = [line.strip() for line in fp.readlines()]\n",
    "word2int = {k:i for i, k in enumerate(words_all)}\n",
    "VOCAB_SIZE = len(word2int) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open('pos/tags.txt') as fp:\n",
    "    tags_all = [line.strip() for line in fp.readlines()]\n",
    "tag2int = {k:i for i, k in enumerate(tags_all)}\n",
    "TAGS_NO = len(tag2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "pos_tagger = load_model('models/pos_lstm_3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 50)]              0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 50, 100)           7840700   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 50, 160)          115840    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 50, 34)           5474      \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,962,014\n",
      "Trainable params: 7,962,014\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "pos_tagger.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "SEQ_LEN = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_chars = [\n",
    "    'آ', 'أ', 'ؤ', 'إ', 'ئ', 'ا', 'ب', 'ة', 'ت', 'ث', 'ج', 'ح', 'خ', 'د', 'ذ', 'ر', 'ز', 'س', 'ش',\n",
    "    'ص','ض','ط','ظ','ع','غ','ف','ق','ك','ل','م','ن','ه','و','ى','ي','٠','١','٢','٣', '٤', '٥', '٦', '٧',\n",
    "    '٨', '٩', 'چ', 'ژ', 'ک', 'گ', 'ھ', 'ی', '۰', '۱', '۲', '۳', '۴', '۵', '۶', '۷', '۸', '۹',\n",
    "    '\\u200c', '\\u200d', '\\u200e', '\\u200f',# 'ٹ', 'څ', 'ڈ', 'ڑ', 'ڕ',\n",
    "    'ﭼ', 'ﯽ', 'ﯾ', 'ﯿ', 'ﷲ', 'ﺀ', 'ﺄ', 'ﺆ', 'ﺋ', 'ﺎ', 'ﺑ', 'ﺔ', 'ﺗ', 'ﺘ', 'ﺧ', 'ﺪ', 'ﺮ', 'ﺳ', 'ﺴ', 'ﺿ',\n",
    "    'ﻋ','ﻌ', 'ﻗ', 'ﻠ', 'ﻣ', 'ﻨ', 'ﻼ', '￼', 'پ',]\n",
    "\n",
    "trans_chars = [\n",
    "    'ً', 'ٌ', 'ٍ', 'َ', 'ُ', 'ِ', 'ّ', 'ْ', 'ٓ', 'ٔ',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from hazm import Normalizer\n",
    "\n",
    "normalizer = Normalizer()\n",
    "\n",
    "def normalize_text(text):\n",
    "    text = normalizer.normalize(text)\n",
    "    text = ' '.join([a.strip() for a in re.split(\"([۰-۹]+)\", text) if a])\n",
    "    text = re.sub('[' + ''.join(trans_chars) + ']', '', text)\n",
    "    text = re.sub('[^' + ''.join(allowed_chars) + ']', ' ', text)\n",
    "#     text = re.sub('ئ', 'ی', text)\n",
    "    text = re.sub('ء', '', text)\n",
    "    text = re.sub('[\\s]+', ' ', text)\n",
    "    return text\n",
    "\n",
    "def encode_text(text):\n",
    "    text = normalize_text(text)\n",
    "    tokens = nltk.tokenize.word_tokenize(text)\n",
    "    return tokens, [word2int[word] if word in word2int else word2int['[UNK]'] for word in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "def pos_tag(text):\n",
    "    test_X = []\n",
    "    test_words = []\n",
    "\n",
    "    text_words, text_tokens = encode_text(text)\n",
    "    words_list = list(chunks(text_words, SEQ_LEN))\n",
    "    tokens_list = list(chunks(text_tokens, SEQ_LEN))\n",
    "    test_X += tokens_list\n",
    "    test_words += words_list\n",
    "    test_X = pad_sequences(test_X, maxlen=SEQ_LEN, padding='post')\n",
    "    pred_outs = pos_tagger.predict(test_X)\n",
    "    pred_args = np.argmax(pred_outs, axis=2)\n",
    "    pred_tags = []\n",
    "    for i, pred in enumerate(pred_args):\n",
    "        cur_tags = [tags_all[i] if i in range(len(tags_all)) else 'UNK' for i in pred]\n",
    "        cur_pairs = list(zip(test_words[i], cur_tags))\n",
    "        pred_tags += cur_pairs\n",
    "    return pred_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tag Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('اصل', 'N_SING'), ('موضوع', 'N_SING'), ('اجتماع', 'N_SING'), ('بیان', 'N_SING'), ('می\\u200cکند', 'V_PRS'), ('۱', 'N_SING'), ('یا', 'CON'), ('به', 'P'), ('بیان', 'N_SING'), ('دیگر', 'ADJ'), ('برای', 'P'), ('هر', 'DET'), ('دسته', 'N_SING'), ('دلخواه', 'ADJ'), ('از', 'P'), ('مجموعه\\u200cها', 'N_PL'), ('مجموعه\\u200cای', 'N_SING'), ('وجود', 'N_SING'), ('دارد', 'V_PRS'), ('که', 'CON'), ('شامل', 'ADJ'), ('همه', 'PRO'), ('عناصری', 'N_PL'), ('است', 'V_PRS'), ('که', 'CON'), ('حداقل', 'ADV'), ('به', 'P'), ('یکی', 'PRO'), ('از', 'P'), ('مجموعه\\u200cهای', 'N_PL'), ('دسته', 'N_SING'), ('مفروض', 'N_SING'), ('متعلق', 'ADJ'), ('باشند', 'V_SUB'), ('به', 'P'), ('بیان', 'N_SING'), ('دیگر', 'ADJ'), ('اگر', 'CON'), ('دسته\\u200cای', 'N_SING'), ('از', 'P'), ('مجموعه\\u200cها', 'N_PL'), ('باشد', 'V_SUB'), ('مجموعه\\u200cای', 'N_SING'), ('چون', 'CON'), ('وجود', 'N_SING'), ('دارد', 'V_PRS'), ('که', 'CON'), ('اگر', 'CON'), ('موجود', 'ADJ'), ('باشد', 'V_SUB'), ('به\\u200cطوری\\u200cکه', 'N_SING'), ('آنگاه', 'ADV'), ('۲', 'N_SING'), ('اما', 'CON'), ('توجه', 'N_SING'), ('داشته', 'V_PP'), ('باشید', 'V_SUB'), ('که', 'CON'), ('مجموعه', 'N_SING'), ('فراگیر', 'ADJ'), ('که', 'CON'), ('تا', 'P'), ('به', 'P'), ('حال', 'N_SING'), ('وجود', 'N_SING'), ('آن', 'PRO'), ('را', 'CLITIC'), ('بر', 'P'), ('اساس', 'N_SING'), ('اصل', 'N_SING'), ('موضوع', 'N_SING'), ('اجتماع', 'N_SING'), ('تضمین', 'N_SING'), ('کرده\\u200cایم', 'V_PP'), ('ممکن', 'ADJ'), ('است', 'V_PRS'), ('بیش', 'ADJ'), ('از', 'P'), ('مورد', 'N_SING'), ('نیاز', 'N_SING'), ('فراگیر', 'ADJ'), ('باشد', 'V_SUB'), ('و', 'CON'), ('شامل', 'ADJ'), ('عناصری', 'N_PL'), ('باشد', 'V_SUB'), ('که', 'CON'), ('به', 'P'), ('هیچ\\u200cیک', 'PRO'), ('از', 'P'), ('عناصر', 'N_PL'), ('در', 'P'), ('دسته', 'N_SING'), ('متعلق', 'ADJ'), ('نباشند', 'V_SUB'), ('چرا', 'ADV_I'), ('که', 'CON'), ('اصل', 'N_SING'), ('موضوع', 'N_SING'), ('اجتماع', 'N_SING'), ('بیان', 'N_SING'), ('می\\u200cکند', 'V_PRS'), ('شامل', 'ADJ'), ('عناصر', 'N_PL'), ('مجموعه\\u200cهای', 'N_PL'), ('در', 'P'), ('است', 'V_PRS'), ('ولی', 'CON'), ('تضمین', 'N_SING'), ('نمی\\u200cکند', 'V_PRS'), ('که', 'CON'), ('این', 'DET'), ('مجموعه', 'N_SING'), ('شامل', 'ADJ'), ('اعضای', 'N_PL'), ('دیگری', 'ADJ'), ('نیست', 'V_PRS'), ('برای', 'P'), ('رفع', 'N_SING'), ('این', 'DET'), ('مشکل', 'N_SING'), ('و', 'CON'), ('ایجاد', 'N_SING'), ('مجموعه\\u200cای', 'N_SING'), ('که', 'CON'), ('دقیقا', 'ADV'), ('شامل', 'ADJ'), ('همه', 'PRO'), ('عناصری', 'N_PL'), ('باشد', 'V_SUB'), ('که', 'CON'), ('به', 'P'), ('حداقل', 'ADV'), ('یکی', 'PRO'), ('از', 'P'), ('مجموعه\\u200cهای', 'N_PL'), ('دسته', 'N_SING'), ('متعلق', 'ADJ'), ('باشند', 'V_SUB'), ('کافی', 'ADJ'), ('است', 'V_PRS'), ('اصل', 'N_SING'), ('موضوع', 'N_SING'), ('تصریح', 'N_SING'), ('را', 'CLITIC'), ('به', 'P'), ('کار', 'N_SING'), ('گرفته', 'V_PP'), ('و', 'CON'), ('مجموعه', 'N_SING'), ('به', 'P'), ('ازا', 'N_SING'), ('یک', 'NUM'), ('را', 'CLITIC'), ('تشکیل', 'N_SING'), ('دهیم', 'V_SUB'), ('در', 'P'), ('این', 'DET'), ('صورت', 'N_SING'), ('شرط', 'N_SING'), ('لازم', 'ADJ'), ('و', 'CON'), ('کافی', 'ADJ'), ('برای', 'P'), ('اینکه', 'N_SING'), ('ای', 'N_SING'), ('متعلق', 'ADJ'), ('به', 'P'), ('این', 'DET'), ('مجموعه', 'N_SING'), ('باشد', 'V_SUB'), ('این', 'PRO'), ('است', 'V_PRS'), ('که', 'CON'), ('ای', 'INT'), ('موجود', 'ADJ'), ('باشد', 'V_SUB'), ('که', 'CON'), ('یعنی', 'CON'), ('به', 'P'), ('حداقل', 'ADV'), ('یکی', 'PRO'), ('از', 'P'), ('مجموعه\\u200cهای', 'N_PL'), ('دسته', 'N_SING'), ('متعلق', 'ADJ'), ('باشد', 'V_SUB'), ('به', 'P'), ('زبان', 'N_SING'), ('منطق', 'N_SING'), ('ریاضی', 'N_SING'), ('داریم', 'V_PRS'), ('۳', 'N_SING'), ('اصل', 'N_SING'), ('موضوع', 'N_SING'), ('گسترش', 'N_SING'), ('یگانگی', 'N_SING'), ('این', 'DET'), ('مجموعه', 'N_SING'), ('را', 'CLITIC'), ('تضمین', 'N_SING'), ('می\\u200cکند', 'V_PRS'), ('و', 'CON'), ('لذا', 'CON'), ('می\\u200cتوان', 'V_AUX'), ('برای', 'P'), ('آن', 'PRO'), ('نام', 'N_SING'), ('و', 'CON'), ('نماد', 'N_SING'), ('مخصوصی', 'ADJ'), ('را', 'CLITIC'), ('اختصاص', 'N_SING'), ('داد', 'V_PA'), ('این', 'DET'), ('مجموعه', 'N_SING'), ('را', 'CLITIC'), ('اجتماع', 'N_SING'), ('دسته', 'N_SING'), ('از', 'P'), ('مجموعه\\u200cها', 'N_PL'), ('می\\u200cخوانیم', 'V_PRS'), ('و', 'CON'), ('با', 'P'), ('نمادهای', 'N_PL'), ('۴', 'ADJ'), ('و', 'CON'), ('۵', 'N_SING'), ('نمایش', 'N_SING'), ('می\\u200cدهیم', 'V_PRS'), ('و', 'CON'), ('مطابق', 'ADJ'), ('تعریف', 'N_SING'), ('۶', 'N_SING'), ('اگر', 'CON'), ('دسته', 'N_SING'), ('از', 'P'), ('مجموعه\\u200cها', 'N_PL'), ('تهی', 'ADJ'), ('باشد', 'V_SUB'), ('در', 'P'), ('این', 'DET'), ('صورت', 'N_SING'), ('مطابق', 'ADJ'), ('تعریف', 'N_SING'), ('اجتماع', 'N_SING'), ('آن', 'PRO'), ('نیز', 'CON'), ('تهی', 'ADJ'), ('خواهد', 'V_AUX'), ('بود', 'V_PA'), ('پس', 'ADV'), ('اجتماع', 'N_SING'), ('دسته\\u200cای', 'N_SING'), ('تهی', 'ADJ'), ('از', 'P'), ('مجموعه\\u200cها', 'N_PL'), ('تهی', 'ADJ'), ('است', 'V_PRS'), ('همچنین', 'CON'), ('اگر', 'CON'), ('و', 'CON'), ('دو', 'NUM'), ('مجموعه', 'N_SING'), ('باشند', 'V_SUB'), ('دسته', 'N_SING'), ('بنابر', 'N_SING'), ('اصل', 'N_SING'), ('موضوع', 'N_SING'), ('زوج', 'N_SING'), ('سازی', 'ADJ'), ('یک', 'NUM'), ('مجموعه', 'N_SING'), ('است', 'V_PRS'), ('و', 'CON'), ('نیز', 'CON'), ('بنابر', 'N_SING'), ('اصل', 'N_SING'), ('موضوع', 'N_SING'), ('اجتماع', 'N_SING'), ('مجموعه', 'N_SING'), ('شامل', 'ADJ'), ('همه', 'PRO'), ('عناصری', 'N_PL'), ('که', 'CON'), ('به', 'P'), ('حداقل', 'ADV'), ('یکی', 'PRO'), ('از', 'P'), ('مجموعه\\u200cهای', 'N_PL'), ('متعلق', 'ADJ'), ('می\\u200cباشند', 'V_PRS'), ('وجود', 'N_SING'), ('دارد', 'V_PRS'), ('در', 'P'), ('این', 'DET'), ('صورت', 'N_SING'), ('اجتماع', 'N_SING'), ('دسته', 'N_SING'), ('را', 'CLITIC'), ('اجتماع', 'N_SING'), ('دو', 'NUM'), ('مجموعه', 'N_SING'), ('و', 'CON'), ('می\\u200cگوییم', 'V_PRS'), ('و', 'CON'), ('آن', 'PRO'), ('را', 'CLITIC'), ('به', 'P'), ('صورت', 'N_SING'), ('نشان', 'N_SING'), ('می\\u200cدهیم', 'V_PRS'), ('و', 'CON'), ('داریم', 'V_PRS'), ('۷', 'N_SING'), ('پس', 'ADV'), ('بنا', 'P'), ('به', 'P'), ('تعریف', 'N_SING'), ('و', 'CON'), ('لذا', 'CON'), ('داریم', 'V_PRS'), ('۸', 'N_SING'), ('با', 'P'), ('توجه', 'N_SING'), ('به', 'P'), ('تعاریف', 'N_PL'), ('فوق', 'ADJ'), ('روابط', 'N_PL'), ('زیر', 'P'), ('را', 'CLITIC'), ('داریم', 'V_PRS'), ('اثبات', 'N_SING'), ('این', 'DET'), ('روابط', 'N_PL'), ('با', 'P'), ('استفاده', 'N_SING'), ('از', 'P'), ('تعاریف', 'N_PL'), ('ساده', 'ADJ'), ('است', 'V_PRS'), ('البته', 'ADV'), ('بر', 'P'), ('هر', 'DET'), ('علاقه\\u200cمند', 'N_SING'), ('ریاضی', 'N_SING'), ('واجب', 'ADJ'), ('است', 'V_PRS'), ('که', 'CON'), ('حداقل', 'ADV'), ('یکبار', 'N_SING'), ('آن\\u200cها', 'PRO'), ('را', 'CLITIC'), ('اثبات', 'N_SING'), ('کند', 'V_SUB'), ('اجتماع', 'N_SING'), ('خاصیت', 'N_SING'), ('جابجایی', 'N_SING'), ('دارد', 'V_PRS'), ('اجتماع', 'N_SING'), ('خاصیت', 'N_SING'), ('شرکت\\u200cپذیری', 'N_SING'), ('دارد', 'V_PRS'), ('اجتماع', 'N_SING'), ('خاصیت', 'N_SING'), ('خود', 'PRO'), ('توانی', 'N_SING'), ('دارد', 'V_PRS'), ('اگر', 'CON'), ('و', 'CON'), ('فقط', 'ADV'), ('اگر', 'CON')]\n"
     ]
    }
   ],
   "source": [
    "from random import sample\n",
    "\n",
    "with open('../dataset/IR_dataset/78.txt') as fp:\n",
    "    text = fp.read()\n",
    "    text = re.sub('\\n', ' ', text)\n",
    "    print(pos_tag(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "word2vec = np.load(\"w2v/word2vec2.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('w2v/vocab2.txt') as fp:\n",
    "    vocab = [l.strip() for l in fp.readlines()]\n",
    "vocab_r = {k:i for i, k in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75011, (75011, 200))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab), word2vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word2vec(word):\n",
    "    return word2vec[vocab_r[word]] if word in vocab_r else np.zeros((EMBED_DIM,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tag Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_w = {\n",
    "    'PAD': 0,\n",
    "    'ADJ': 1,\n",
    "    'ADJ_CMPR': 1,\n",
    "    'ADJ_INO': 1,\n",
    "    'ADJ_SUP': 1,\n",
    "    'ADJ_VOC': 1,\n",
    "    'ADV': 0.5,\n",
    "    'ADV_COMP': 0.5,\n",
    "    'ADV_I': 0.5,\n",
    "    'ADV_LOC': 0.5,\n",
    "    'ADV_NEG': 0.5,\n",
    "    'ADV_TIME': 0.5,\n",
    "    'CLITIC': 0,\n",
    "    'CON': 0,\n",
    "    'DELM': 0,\n",
    "    'DET': 0,\n",
    "    'FW': 2,\n",
    "    'INT': 0,\n",
    "    'NUM': 2,\n",
    "    'N_PL': 6,\n",
    "    'N_SING': 6,\n",
    "    'N_VOC': 6,\n",
    "    'P': 0,\n",
    "    'PREV': 0,\n",
    "    'PRO': 0,\n",
    "    'SYM': 0,\n",
    "    'UNK': 0,\n",
    "    'V_AUX': 0,\n",
    "    'V_IMP': 0,\n",
    "    'V_PA': 0,\n",
    "    'V_PP': 0,\n",
    "    'V_PRS': 0,\n",
    "    'V_SUB': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-Idf Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import load_npz\n",
    "\n",
    "tfidf = load_npz('tfidf/tfidf.npz')\n",
    "\n",
    "tfidf_words = []\n",
    "with open('tfidf/words.txt') as fp:\n",
    "    tfidf_words = [w.strip() for w in fp.readlines()]\n",
    "tfidf_words_r = {k:i for i, k in enumerate(tfidf_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30680919856295225"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf[docs_r['../dataset/IR_dataset/2048.txt'], tfidf_words_r['پودر']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf(doc, word):\n",
    "    return tfidf[docs_r[doc], tfidf_words_r[word]] if word in tfidf_words_r else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_doc(doc, text, tag_wi=tag_w, has_tfidf=True):\n",
    "    pos_tags = pos_tag(text)\n",
    "    vectors = np.zeros((len(pos_tags), EMBED_DIM), dtype=np.float64)\n",
    "    weights = np.zeros((len(pos_tags),), dtype=np.float64)\n",
    "    i = 0\n",
    "    for word, tag in pos_tags:\n",
    "        vectors[i] = get_word2vec(word)\n",
    "        weights[i] = tag_wi[tag]\n",
    "        if has_tfidf:\n",
    "            weights[i] += get_tfidf(doc, word) # To be tuned\n",
    "        i += 1\n",
    "        \n",
    "    s = softmax(weights)\n",
    "    return np.dot(s, vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(a, b):\n",
    "    na = np.linalg.norm(a)\n",
    "    nb =np.linalg.norm(b) \n",
    "    return np.dot(a, b) / (na * nb) if na > 0 and nb > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_query(query):\n",
    "    pos_tags = pos_tag(query)\n",
    "    vectors = np.zeros((len(pos_tags), EMBED_DIM), dtype=np.float64)\n",
    "    weights = np.zeros((len(pos_tags),), dtype=np.float64)\n",
    "    i = 0\n",
    "    for word, tag in pos_tags:\n",
    "        vectors[i] = get_word2vec(word)\n",
    "        weights[i] = 1 / len(pos_tags)\n",
    "        i += 1\n",
    "    return np.dot(weights, vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_docs(tag_wi=tag_w, has_tfidf=True):\n",
    "    doc_vectors = np.zeros((len(docs), EMBED_DIM), dtype=np.float64)\n",
    "    for i, doc in enumerate(docs):\n",
    "        with open(doc) as fp:\n",
    "            text = fp.read()\n",
    "            text = re.sub('\\n', ' ', text)\n",
    "        try:\n",
    "            doc_vectors[i] = embed_doc(doc, text, tag_wi, has_tfidf)\n",
    "        except:\n",
    "            print('Error', doc, 'at', i, 'occurred.')\n",
    "    return doc_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ir_measures as IRm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "1tXZutKIhv7V"
   },
   "outputs": [],
   "source": [
    "MRR = IRm.measures.MRR()\n",
    "def mrr_measure(qrels, outputs):\n",
    "    ret = pd.DataFrame(outputs, columns=['query_id', 'doc_id', 'score'])\n",
    "    return MRR.calc_aggregate(qrels[qrels.relevance == 4], ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "J4z0Ti8eldAH"
   },
   "outputs": [],
   "source": [
    "def map_measure(qrels, outputs):\n",
    "    ret = pd.DataFrame(outputs, columns=['query_id', 'doc_id', 'score'])\n",
    "    return np.mean([IRm.measures.AP(rel=level).\\\n",
    "                    calc_aggregate(qrels[qrels.relevance == level], ret) for level in range(1,4+1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "aq1ThwOfUFt2"
   },
   "outputs": [],
   "source": [
    "def p_measure(qrels, outputs):\n",
    "    ret = pd.DataFrame(outputs, columns=['query_id', 'doc_id', 'score'])\n",
    "    return np.mean([IRm.measures.P(cutoff=k, rel=level).\\\n",
    "                    calc_aggregate(qrels[qrels.relevance == level], ret)\\\n",
    "                  for k,level in zip([1,11,21,31],range(1,4+1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve & Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error ../dataset/IR_dataset/2885.txt at 569 occurred.\n"
     ]
    }
   ],
   "source": [
    "doc_vectors = embed_docs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_id(doc):\n",
    "    return int(doc.split('/')[-1].split('.')[0])\n",
    "\n",
    "def retrieve_docs(query, doc_vectors):\n",
    "    q_vec = embed_query(query)\n",
    "    scores = list(zip([query for _ in range(len(docs))],\n",
    "                      [str(get_doc_id(d)) for d in docs], \n",
    "                      np.dot(doc_vectors, q_vec)))\n",
    "    return sorted(scores, key=lambda x: -x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR: 0.2402969847760753\n",
      "MAP: 0.16490216535458788\n",
      "P@k: 0.07264790764790764\n"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "\n",
    "for query in dataset:\n",
    "    #print('Q:', query)\n",
    "    #print(dataset_dev[query])\n",
    "    retrieved = retrieve_docs(query, doc_vectors)[:100]\n",
    "    outputs += retrieved\n",
    "print('MRR:', mrr_measure(qrels, outputs))\n",
    "print('MAP:', map_measure(qrels, outputs))\n",
    "print('P@k:', p_measure(qrels, outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag weight: 3 Tf-idf: T\n",
      "Error ../dataset/IR_dataset/2885.txt at 569 occurred.\n",
      "MRR: 0.21411914882503114\n",
      "MAP: 0.15861841426376194\n",
      "P@k: 0.07136689475399151\n",
      "Tag weight: 3 Tf-idf: F\n",
      "Error ../dataset/IR_dataset/2885.txt at 569 occurred.\n",
      "MRR: 0.2231447447826758\n",
      "MAP: 0.1426937241899583\n",
      "P@k: 0.06707396546106223\n",
      "Tag weight: 6 Tf-idf: T\n",
      "Error ../dataset/IR_dataset/2885.txt at 569 occurred.\n",
      "MRR: 0.22087218337218334\n",
      "MAP: 0.15974856968562778\n",
      "P@k: 0.07255737094446771\n",
      "Tag weight: 6 Tf-idf: F\n",
      "Error ../dataset/IR_dataset/2885.txt at 569 occurred.\n",
      "MRR: 0.2175260178306155\n",
      "MAP: 0.14092246351854398\n",
      "P@k: 0.06925475957734022\n",
      "Tag weight: 9 Tf-idf: T\n",
      "Error ../dataset/IR_dataset/2885.txt at 569 occurred.\n",
      "MRR: 0.22007853257853255\n",
      "MAP: 0.1586301014453418\n",
      "P@k: 0.07179979518689195\n",
      "Tag weight: 9 Tf-idf: F\n",
      "Error ../dataset/IR_dataset/2885.txt at 569 occurred.\n",
      "MRR: 0.21345397875857644\n",
      "MAP: 0.13813564618263247\n",
      "P@k: 0.06925475957734022\n",
      "Tag weight: 12 Tf-idf: T\n",
      "Error ../dataset/IR_dataset/2885.txt at 569 occurred.\n",
      "MRR: 0.21063408813408813\n",
      "MAP: 0.15593566920775942\n",
      "P@k: 0.07100614439324114\n",
      "Tag weight: 12 Tf-idf: F\n",
      "Error ../dataset/IR_dataset/2885.txt at 569 occurred.\n",
      "MRR: 0.20893738424198194\n",
      "MAP: 0.13632348911312245\n",
      "P@k: 0.06810035842293906\n",
      "Tag weight: 18 Tf-idf: T\n",
      "Error ../dataset/IR_dataset/2885.txt at 569 occurred.\n",
      "MRR: 0.21063408813408813\n",
      "MAP: 0.15518691564650586\n",
      "P@k: 0.07100614439324114\n",
      "Tag weight: 18 Tf-idf: F\n",
      "Error ../dataset/IR_dataset/2885.txt at 569 occurred.\n",
      "MRR: 0.20893738424198194\n",
      "MAP: 0.13632348911312245\n",
      "P@k: 0.06810035842293906\n"
     ]
    }
   ],
   "source": [
    "for w in [3, 6, 9, 12, 18]:\n",
    "    tag_w['N_PL'] = tag_w['N_SING'] = w\n",
    "    print('Tag weight:', w, 'Tf-idf: T')\n",
    "    doc_vectors = embed_docs(tag_w, has_tfidf=True)\n",
    "    outputs = []\n",
    "    for query in dataset_dev:\n",
    "        retrieved = retrieve_docs(query, doc_vectors)[:30]\n",
    "        outputs += retrieved\n",
    "    print('MRR:', mrr_measure(qrels, outputs))\n",
    "    print('MAP:', map_measure(qrels, outputs))\n",
    "    print('P@k:', p_measure(qrels, outputs))\n",
    "    \n",
    "    print('Tag weight:', w, 'Tf-idf: F')\n",
    "    doc_vectors = embed_docs(tag_w, has_tfidf=False)\n",
    "    outputs = []\n",
    "    for query in dataset_dev:\n",
    "        retrieved = retrieve_docs(query, doc_vectors)[:30]\n",
    "        outputs += retrieved\n",
    "    print('MRR:', mrr_measure(qrels, outputs))\n",
    "    print('MAP:', map_measure(qrels, outputs))\n",
    "    print('P@k:', p_measure(qrels, outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(list(zip([int(d.split('/')[-1].split('.')[0]) for d in docs], \n",
    "                np.dot(doc_vectors, q_vec) / (np.linalg.norm(q_vec) * np.linalg.norm(doc_vectors, axis=1)))), \n",
    "                key=lambda x: -x[1])[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(list(zip([int(d.split('/')[-1].split('.')[0]) for d in docs], np.dot(doc_vectors, q_vec))), \n",
    "       key=lambda x: -x[1])[:20]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
