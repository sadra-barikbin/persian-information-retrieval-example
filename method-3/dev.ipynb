{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-28T18:07:26.055600Z",
     "iopub.status.busy": "2021-12-28T18:07:26.055202Z",
     "iopub.status.idle": "2021-12-28T18:07:26.070350Z",
     "shell.execute_reply": "2021-12-28T18:07:26.069255Z",
     "shell.execute_reply.started": "2021-12-28T18:07:26.055536Z"
    }
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "with open('docs.txt') as fp:\n",
    "    docs = [d.strip() for d in fp.readlines()]\n",
    "docs_r = {k:i for i, k in enumerate(docs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../dataset/IR_dataset/2048.txt',\n",
       " '../dataset/IR_dataset/2404.txt',\n",
       " '../dataset/IR_dataset/661.txt',\n",
       " '../dataset/IR_dataset/1252.txt',\n",
       " '../dataset/IR_dataset/726.txt',\n",
       " '../dataset/IR_dataset/3029.txt',\n",
       " '../dataset/IR_dataset/329.txt',\n",
       " '../dataset/IR_dataset/1481.txt',\n",
       " '../dataset/IR_dataset/1511.txt',\n",
       " '../dataset/IR_dataset/1127.txt',\n",
       " '../dataset/IR_dataset/2447.txt',\n",
       " '../dataset/IR_dataset/2907.txt',\n",
       " '../dataset/IR_dataset/630.txt',\n",
       " '../dataset/IR_dataset/3008.txt',\n",
       " '../dataset/IR_dataset/494.txt',\n",
       " '../dataset/IR_dataset/321.txt',\n",
       " '../dataset/IR_dataset/40.txt',\n",
       " '../dataset/IR_dataset/3131.txt',\n",
       " '../dataset/IR_dataset/2192.txt',\n",
       " '../dataset/IR_dataset/2167.txt']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dev Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-28T18:07:32.399534Z",
     "iopub.status.busy": "2021-12-28T18:07:32.399308Z",
     "iopub.status.idle": "2021-12-28T18:07:32.449512Z",
     "shell.execute_reply": "2021-12-28T18:07:32.448903Z",
     "shell.execute_reply.started": "2021-12-28T18:07:32.399511Z"
    }
   },
   "outputs": [],
   "source": [
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-28T18:07:34.125471Z",
     "iopub.status.busy": "2021-12-28T18:07:34.125218Z",
     "iopub.status.idle": "2021-12-28T18:07:34.682256Z",
     "shell.execute_reply": "2021-12-28T18:07:34.680977Z",
     "shell.execute_reply.started": "2021-12-28T18:07:34.125445Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('../dataset/evaluation_IR.yml', 'r') as f:\n",
    "    dataset = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-28T18:07:35.844057Z",
     "iopub.status.busy": "2021-12-28T18:07:35.843692Z",
     "iopub.status.idle": "2021-12-28T18:07:35.857803Z",
     "shell.execute_reply": "2021-12-28T18:07:35.853698Z",
     "shell.execute_reply.started": "2021-12-28T18:07:35.844009Z"
    }
   },
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-28T18:07:37.830065Z",
     "iopub.status.busy": "2021-12-28T18:07:37.829671Z",
     "iopub.status.idle": "2021-12-28T18:07:37.835147Z",
     "shell.execute_reply": "2021-12-28T18:07:37.834230Z",
     "shell.execute_reply.started": "2021-12-28T18:07:37.830021Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_dev = {k:v for k, v in list(dataset.items())[-30:]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['نظریه اصل موضوعی مجموعه\\u200cها  سازگاری و عدم وابستگی در ZFC',\n",
       " 'نظریه نسبیت  نسبیت عام',\n",
       " 'نلسون ماندلا  آغاز',\n",
       " 'نهنگ قاتل  هوش',\n",
       " 'نوا (دستگاه موسیقی)  گوشه\\u200cها',\n",
       " 'نیروهای محور  اتحاد دانوب، اختلاف بر سر اتریش',\n",
       " 'هاینریش هیملر  استحکام قدرت',\n",
       " 'هاینریش هیملر  رابطه با هیتلر',\n",
       " 'هسته لینوکس',\n",
       " 'هسته لینوکس  درگیری\\u200cهای جامعه توسعه',\n",
       " 'هسته لینوکس  مدل توسعه',\n",
       " 'هم\\u200cارزی جرم و انرژی  کاربست\\u200cپذیری فرمول',\n",
       " 'هندسه جبری',\n",
       " 'هوش مصنوعی  تاریخچه',\n",
       " 'واپاشی هسته\\u200cای  پایداری و ناپایداری ایزوتوپ\\u200cها',\n",
       " 'وشمگیر  وضعیت سیاسی-اجتماعی قرن چهارم هجری',\n",
       " 'ولایت قندهار  تمدن مندیگک',\n",
       " 'ولفگانگ آمادئوس موتسارت  موتسارت در وین',\n",
       " 'ونکوور  سیستم حمل و نقل شهری',\n",
       " 'ونکوور  معماری',\n",
       " 'پروین اعتصامی',\n",
       " 'پرچم ایران  پیش از پادشاهی پهلوی\\u200cها',\n",
       " 'پیمان کیوتو  اتحادیه اروپا',\n",
       " 'چرخه آب  توصیف',\n",
       " 'چنگیز خان  کودکی',\n",
       " 'چهاردهمین دالایی لاما  اوان زندگی و سابقه',\n",
       " 'کارل مارکس  اقتصاد، تاریخ و جامعه',\n",
       " 'گرجستان  تاریخ',\n",
       " 'یانی  تأثیرپذیری\\u200cهای موسیقایی',\n",
       " 'یونسکو  فعالیت\\u200cها']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dataset_dev.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-28T18:07:58.280925Z",
     "iopub.status.busy": "2021-12-28T18:07:58.280313Z",
     "iopub.status.idle": "2021-12-28T18:08:03.762250Z",
     "shell.execute_reply": "2021-12-28T18:08:03.761641Z",
     "shell.execute_reply.started": "2021-12-28T18:07:58.280851Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-12-28T18:08:03.765429Z",
     "iopub.status.busy": "2021-12-28T18:08:03.765188Z",
     "iopub.status.idle": "2021-12-28T18:08:03.830735Z",
     "shell.execute_reply": "2021-12-28T18:08:03.829874Z",
     "shell.execute_reply.started": "2021-12-28T18:08:03.765404Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open('pos/words.txt') as fp:\n",
    "    words_all = [line.strip() for line in fp.readlines()]\n",
    "word2int = {k:i for i, k in enumerate(words_all)}\n",
    "VOCAB_SIZE = len(word2int) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-12-28T18:08:03.831894Z",
     "iopub.status.busy": "2021-12-28T18:08:03.831690Z",
     "iopub.status.idle": "2021-12-28T18:08:03.837395Z",
     "shell.execute_reply": "2021-12-28T18:08:03.836288Z",
     "shell.execute_reply.started": "2021-12-28T18:08:03.831870Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open('pos/tags.txt') as fp:\n",
    "    tags_all = [line.strip() for line in fp.readlines()]\n",
    "tag2int = {k:i for i, k in enumerate(tags_all)}\n",
    "TAGS_NO = len(tag2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-28T18:08:03.839099Z",
     "iopub.status.busy": "2021-12-28T18:08:03.838562Z",
     "iopub.status.idle": "2021-12-28T18:08:05.982290Z",
     "shell.execute_reply": "2021-12-28T18:08:05.980065Z",
     "shell.execute_reply.started": "2021-12-28T18:08:03.839054Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "pos_tagger = load_model('models/pos_lstm_3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-12-28T18:08:06.451257Z",
     "iopub.status.busy": "2021-12-28T18:08:06.451020Z",
     "iopub.status.idle": "2021-12-28T18:08:06.460204Z",
     "shell.execute_reply": "2021-12-28T18:08:06.459267Z",
     "shell.execute_reply.started": "2021-12-28T18:08:06.451234Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pos_tagger.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "SEQ_LEN = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_chars = [\n",
    "    'آ', 'أ', 'ؤ', 'إ', 'ئ', 'ا', 'ب', 'ة', 'ت', 'ث', 'ج', 'ح', 'خ', 'د', 'ذ', 'ر', 'ز', 'س', 'ش',\n",
    "    'ص','ض','ط','ظ','ع','غ','ف','ق','ك','ل','م','ن','ه','و','ى','ي','٠','١','٢','٣', '٤', '٥', '٦', '٧',\n",
    "    '٨', '٩', 'چ', 'ژ', 'ک', 'گ', 'ھ', 'ی', '۰', '۱', '۲', '۳', '۴', '۵', '۶', '۷', '۸', '۹',\n",
    "    '\\u200c', '\\u200d', '\\u200e', '\\u200f',\n",
    "    'ﭼ', 'ﯽ', 'ﯾ', 'ﯿ', 'ﷲ', 'ﺀ', 'ﺄ', 'ﺆ', 'ﺋ', 'ﺎ', 'ﺑ', 'ﺔ', 'ﺗ', 'ﺘ', 'ﺧ', 'ﺪ', 'ﺮ', 'ﺳ', 'ﺴ', 'ﺿ',\n",
    "    'ﻋ','ﻌ', 'ﻗ', 'ﻠ', 'ﻣ', 'ﻨ', 'ﻼ', '￼', 'پ',]\n",
    "\n",
    "trans_chars = [\n",
    "    'ً', 'ٌ', 'ٍ', 'َ', 'ُ', 'ِ', 'ّ', 'ْ', 'ٓ', 'ٔ',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "def normalize_text(text):\n",
    "    text = re.sub('[' + ''.join(trans_chars) + ']', '', text)\n",
    "    text = re.sub('[^' + ''.join(allowed_chars) + ']', ' ', text)\n",
    "#     text = re.sub('ئ', 'ی', text)\n",
    "    text = re.sub('ء', '', text)\n",
    "    text = re.sub('[\\s]+', ' ', text)\n",
    "    return text\n",
    "\n",
    "def encode_text(text):\n",
    "    text = normalize_text(text)\n",
    "    tokens = nltk.tokenize.word_tokenize(text)\n",
    "    return tokens, [word2int[word] if word in word2int else word2int['[UNK]'] for word in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "def pos_tag(text):\n",
    "    test_X = []\n",
    "    test_words = []\n",
    "\n",
    "    text_words, text_tokens = encode_text(text)\n",
    "    words_list = list(chunks(text_words, SEQ_LEN))\n",
    "    tokens_list = list(chunks(text_tokens, SEQ_LEN))\n",
    "    test_X += tokens_list\n",
    "    test_words += words_list\n",
    "    test_X = pad_sequences(test_X, maxlen=SEQ_LEN, padding='post')\n",
    "    pred_outs = pos_tagger.predict(test_X)\n",
    "    pred_args = np.argmax(pred_outs, axis=2)\n",
    "    pred_tags = []\n",
    "    for i, pred in enumerate(pred_args):\n",
    "        cur_tags = [tags_all[i] if i in range(len(tags_all)) else 'UNK' for i in pred]\n",
    "        cur_pairs = list(zip(test_words[i], cur_tags))\n",
    "        pred_tags += cur_pairs\n",
    "    return pred_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tag Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('اصل', 'N_SING'), ('موضوع', 'N_SING'), ('اجتماع', 'N_SING'), ('بیان', 'N_SING'), ('می\\u200cکند', 'V_PRS'), ('یا', 'CON'), ('به', 'P'), ('بیان', 'N_SING'), ('دیگر', 'ADJ'), ('برای', 'P'), ('هر', 'DET'), ('دسته', 'N_SING'), ('دلخواه', 'ADJ'), ('از', 'P'), ('مجموعه\\u200cها', 'N_PL'), ('مجموعه\\u200cای', 'N_SING'), ('وجود', 'N_SING'), ('دارد', 'V_PRS'), ('که', 'CON'), ('شامل', 'ADJ'), ('همه', 'PRO'), ('عناصری', 'N_PL'), ('است', 'V_PRS'), ('که', 'CON'), ('حداقل', 'ADV'), ('به', 'P'), ('یکی', 'PRO'), ('از', 'P'), ('مجموعه\\u200cهای', 'N_PL'), ('دسته', 'N_SING'), ('مفروض', 'N_SING'), ('متعلق', 'ADJ'), ('باشند', 'V_SUB'), ('به', 'P'), ('بیان', 'N_SING'), ('دیگر', 'ADJ'), ('اگر', 'CON'), ('دسته\\u200cای', 'N_SING'), ('از', 'P'), ('مجموعه\\u200cها', 'N_PL'), ('باشد', 'V_SUB'), ('مجموعه\\u200cای', 'N_SING'), ('چون', 'CON'), ('وجود', 'N_SING'), ('دارد', 'V_PRS'), ('که', 'CON'), ('اگر', 'CON'), ('موجود', 'ADJ'), ('باشد', 'V_SUB'), ('به\\u200cطوری\\u200cکه', 'N_SING'), ('آنگاه', 'ADV'), ('اما', 'CON'), ('توجه', 'N_SING'), ('داشته', 'V_PP'), ('باشید', 'V_SUB'), ('که', 'CON'), ('مجموعه', 'N_SING'), ('فراگیر', 'ADJ'), ('که', 'CON'), ('تا', 'P'), ('به', 'P'), ('حال', 'N_SING'), ('وجود', 'N_SING'), ('آن', 'PRO'), ('را', 'CLITIC'), ('بر', 'P'), ('اساس', 'N_SING'), ('اصل', 'N_SING'), ('موضوع', 'N_SING'), ('اجتماع', 'N_SING'), ('تضمین', 'N_SING'), ('کرده\\u200cایم', 'V_PP'), ('ممکن', 'ADJ'), ('است', 'V_PRS'), ('بیش', 'ADJ'), ('از', 'P'), ('مورد', 'N_SING'), ('نیاز', 'N_SING'), ('فراگیر', 'ADJ'), ('باشد', 'V_SUB'), ('و', 'CON'), ('شامل', 'ADJ'), ('عناصری', 'N_PL'), ('باشد', 'V_SUB'), ('که', 'CON'), ('به', 'P'), ('هیچ\\u200cیک', 'PRO'), ('از', 'P'), ('عناصر', 'N_PL'), ('در', 'P'), ('دسته', 'N_SING'), ('متعلق', 'ADJ'), ('نباشند', 'V_SUB'), ('چرا', 'ADV_I'), ('که', 'CON'), ('اصل', 'N_SING'), ('موضوع', 'N_SING'), ('اجتماع', 'N_SING'), ('بیان', 'N_SING'), ('می\\u200cکند', 'V_PRS'), ('شامل', 'ADJ'), ('عناصر', 'N_PL'), ('مجموعه\\u200cهای', 'N_PL'), ('در', 'P'), ('است', 'V_PRS'), ('ولی', 'CON'), ('تضمین', 'N_SING'), ('نمی\\u200cکند', 'V_PRS'), ('که', 'CON'), ('این', 'DET'), ('مجموعه', 'N_SING'), ('شامل', 'ADJ'), ('اعضای', 'N_PL'), ('دیگری', 'ADJ'), ('نیست', 'V_PRS'), ('برای', 'P'), ('رفع', 'N_SING'), ('این', 'DET'), ('مشکل', 'N_SING'), ('و', 'CON'), ('ایجاد', 'N_SING'), ('مجموعه\\u200cای', 'N_SING'), ('که', 'CON'), ('دقیقا', 'ADV'), ('شامل', 'ADJ'), ('همه', 'PRO'), ('عناصری', 'N_PL'), ('باشد', 'V_SUB'), ('که', 'CON'), ('به', 'P'), ('حداقل', 'ADV'), ('یکی', 'PRO'), ('از', 'P'), ('مجموعه\\u200cهای', 'N_PL'), ('دسته', 'N_SING'), ('متعلق', 'ADJ'), ('باشند', 'V_SUB'), ('کافی', 'ADJ'), ('است', 'V_PRS'), ('اصل', 'N_SING'), ('موضوع', 'N_SING'), ('تصریح', 'N_SING'), ('را', 'CLITIC'), ('به', 'P'), ('کار', 'N_SING'), ('گرفته', 'V_PP'), ('و', 'CON'), ('مجموعه', 'N_SING'), ('به', 'P'), ('ازا', 'N_SING'), ('یک', 'NUM'), ('را', 'CLITIC'), ('تشکیل', 'N_SING'), ('دهیم', 'V_SUB'), ('در', 'P'), ('این', 'DET'), ('صورت', 'N_SING'), ('شرط', 'N_SING'), ('لازم', 'ADJ'), ('و', 'CON'), ('کافی', 'ADJ'), ('برای', 'P'), ('اینکه', 'N_SING'), ('ای', 'N_SING'), ('متعلق', 'ADJ'), ('به', 'P'), ('این', 'DET'), ('مجموعه', 'N_SING'), ('باشد', 'V_SUB'), ('این', 'PRO'), ('است', 'V_PRS'), ('که', 'CON'), ('ای', 'INT'), ('موجود', 'ADJ'), ('باشد', 'V_SUB'), ('که', 'CON'), ('یعنی', 'CON'), ('به', 'P'), ('حداقل', 'ADV'), ('یکی', 'PRO'), ('از', 'P'), ('مجموعه\\u200cهای', 'N_PL'), ('دسته', 'N_SING'), ('متعلق', 'ADJ'), ('باشد', 'V_SUB'), ('به', 'P'), ('زبان', 'N_SING'), ('منطق', 'N_SING'), ('ریاضی', 'N_SING'), ('داریم', 'V_PRS'), ('اصل', 'N_SING'), ('موضوع', 'N_SING'), ('گسترش', 'N_SING'), ('یگانگی', 'N_SING'), ('این', 'DET'), ('مجموعه', 'N_SING'), ('را', 'CLITIC'), ('تضمین', 'N_SING'), ('می\\u200cکند', 'V_PRS'), ('و', 'CON'), ('لذا', 'CON'), ('می\\u200cتوان', 'V_AUX'), ('برای', 'P'), ('آن', 'PRO'), ('نام', 'N_SING'), ('و', 'CON'), ('نماد', 'N_SING'), ('مخصوصی', 'ADJ'), ('را', 'CLITIC'), ('اختصاص', 'N_SING'), ('داد', 'V_PA'), ('این', 'DET'), ('مجموعه', 'N_SING'), ('را', 'CLITIC'), ('اجتماع', 'N_SING'), ('دسته', 'N_SING'), ('از', 'P'), ('مجموعه\\u200cها', 'N_PL'), ('می\\u200cخوانیم', 'V_PRS'), ('و', 'CON'), ('با', 'P'), ('نمادهای', 'N_PL'), ('و', 'CON'), ('نمایش', 'N_SING'), ('می\\u200cدهیم', 'V_PRS'), ('و', 'CON'), ('مطابق', 'ADJ'), ('تعریف', 'N_SING'), ('اگر', 'CON'), ('دسته', 'N_SING'), ('از', 'P'), ('مجموعه\\u200cها', 'N_PL'), ('تهی', 'ADJ'), ('باشد', 'V_SUB'), ('در', 'P'), ('این', 'DET'), ('صورت', 'N_SING'), ('مطابق', 'ADJ'), ('تعریف', 'N_SING'), ('اجتماع', 'N_SING'), ('آن', 'PRO'), ('نیز', 'CON'), ('تهی', 'ADJ'), ('خواهد', 'V_AUX'), ('بود', 'V_PA'), ('پس', 'ADV'), ('اجتماع', 'N_SING'), ('دسته\\u200cای', 'N_SING'), ('تهی', 'ADJ'), ('از', 'P'), ('مجموعه\\u200cها', 'N_PL'), ('تهی', 'ADJ'), ('است', 'V_PRS'), ('همچنین', 'CON'), ('اگر', 'CON'), ('و', 'CON'), ('دو', 'NUM'), ('مجموعه', 'N_SING'), ('باشند', 'V_SUB'), ('دسته', 'N_SING'), ('بنابر', 'N_SING'), ('اصل', 'N_SING'), ('موضوع', 'N_SING'), ('زوج', 'N_SING'), ('سازی', 'ADJ'), ('یک', 'NUM'), ('مجموعه', 'N_SING'), ('است', 'V_PRS'), ('و', 'CON'), ('نیز', 'CON'), ('بنابر', 'N_SING'), ('اصل', 'N_SING'), ('موضوع', 'N_SING'), ('اجتماع', 'N_SING'), ('مجموعه', 'N_SING'), ('شامل', 'ADJ'), ('همه', 'PRO'), ('عناصری', 'N_PL'), ('که', 'CON'), ('به', 'P'), ('حداقل', 'ADV'), ('یکی', 'PRO'), ('از', 'P'), ('مجموعه\\u200cهای', 'N_PL'), ('متعلق', 'ADJ'), ('می\\u200cباشند', 'V_PRS'), ('وجود', 'N_SING'), ('دارد', 'V_PRS'), ('در', 'P'), ('این', 'DET'), ('صورت', 'N_SING'), ('اجتماع', 'N_SING'), ('دسته', 'N_SING'), ('را', 'CLITIC'), ('اجتماع', 'N_SING'), ('دو', 'NUM'), ('مجموعه', 'N_SING'), ('و', 'CON'), ('می\\u200cگوییم', 'V_PRS'), ('و', 'CON'), ('آن', 'PRO'), ('را', 'CLITIC'), ('به', 'P'), ('صورت', 'N_SING'), ('نشان', 'N_SING'), ('می\\u200cدهیم', 'V_PRS'), ('و', 'CON'), ('داریم', 'V_PRS'), ('پس', 'ADV'), ('بنا', 'P'), ('به', 'P'), ('تعریف', 'N_SING'), ('و', 'CON'), ('لذا', 'CON'), ('داریم', 'V_PRS'), ('با', 'P'), ('توجه', 'N_SING'), ('به', 'P'), ('تعاریف', 'N_PL'), ('فوق', 'ADJ'), ('روابط', 'N_PL'), ('زیر', 'P'), ('را', 'CLITIC'), ('داریم', 'V_PRS'), ('اثبات', 'N_SING'), ('این', 'DET'), ('روابط', 'N_PL'), ('با', 'P'), ('استفاده', 'N_SING'), ('از', 'P'), ('تعاریف', 'N_PL'), ('ساده', 'ADJ'), ('است', 'V_PRS'), ('البته', 'ADV'), ('بر', 'P'), ('هر', 'DET'), ('علاقه\\u200cمند', 'N_SING'), ('ریاضی', 'N_SING'), ('واجب', 'ADJ'), ('است', 'V_PRS'), ('که', 'CON'), ('حداقل', 'ADV'), ('یکبار', 'N_SING'), ('آن\\u200cها', 'PRO'), ('را', 'CLITIC'), ('اثبات', 'N_SING'), ('کند', 'V_SUB'), ('اجتماع', 'N_SING'), ('خاصیت', 'N_SING'), ('جابجایی', 'N_SING'), ('دارد', 'V_PRS'), ('اجتماع', 'N_SING'), ('خاصیت', 'N_SING'), ('شرکت\\u200cپذیری', 'N_SING'), ('دارد', 'V_PRS'), ('اجتماع', 'N_SING'), ('خاصیت', 'N_SING'), ('خود', 'PRO'), ('توانی', 'N_SING'), ('دارد', 'V_PRS'), ('اگر', 'CON'), ('و', 'CON'), ('فقط', 'ADV'), ('اگر', 'CON')]\n"
     ]
    }
   ],
   "source": [
    "from random import sample\n",
    "\n",
    "with open('../dataset/IR_dataset/78.txt') as fp:\n",
    "    text = fp.read()\n",
    "    text = re.sub('\\n', ' ', text)\n",
    "    print(pos_tag(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "word2vec = np.load(\"w2v/word2vec2.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('w2v/vocab2.txt') as fp:\n",
    "    vocab = [l.strip() for l in fp.readlines()]\n",
    "vocab_r = {k:i for i, k in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75011, (75011, 200))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab), word2vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word2vec(word):\n",
    "    return word2vec[vocab_r[word]] if word in vocab_r else np.zeros((EMBED_DIM,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tag Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_w = {\n",
    "    'PAD': 0,\n",
    "    'ADJ': 1,\n",
    "    'ADJ_CMPR': 1,\n",
    "    'ADJ_INO': 1,\n",
    "    'ADJ_SUP': 1,\n",
    "    'ADJ_VOC': 1,\n",
    "    'ADV': 0.5,\n",
    "    'ADV_COMP': 0.5,\n",
    "    'ADV_I': 0.5,\n",
    "    'ADV_LOC': 0.5,\n",
    "    'ADV_NEG': 0.5,\n",
    "    'ADV_TIME': 0.5,\n",
    "    'CLITIC': 0,\n",
    "    'CON': 0,\n",
    "    'DELM': 0,\n",
    "    'DET': 0,\n",
    "    'FW': 2,\n",
    "    'INT': 0,\n",
    "    'NUM': 2,\n",
    "    'N_PL': 12,\n",
    "    'N_SING': 12,\n",
    "    'N_VOC': 6,\n",
    "    'P': 0,\n",
    "    'PREV': 0,\n",
    "    'PRO': 0,\n",
    "    'SYM': 0,\n",
    "    'UNK': 0,\n",
    "    'V_AUX': 0,\n",
    "    'V_IMP': 0,\n",
    "    'V_PA': 0,\n",
    "    'V_PP': 0,\n",
    "    'V_PRS': 0,\n",
    "    'V_SUB': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-Idf Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import load_npz\n",
    "\n",
    "tfidf = load_npz('tfidf/tfidf.npz')\n",
    "\n",
    "tfidf_words = []\n",
    "with open('tfidf/words.txt') as fp:\n",
    "    tfidf_words = [w.strip() for w in fp.readlines()]\n",
    "tfidf_words_r = {k:i for i, k in enumerate(tfidf_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30680919856295225"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf[docs_r['../dataset/IR_dataset/2048.txt'], tfidf_words_r['پودر']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf(doc, word):\n",
    "    return tfidf[docs_r[doc], tfidf_words_r[word]] if word in tfidf_words_r else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_doc(doc, text):\n",
    "    pos_tags = pos_tag(text)\n",
    "    vectors = np.zeros((len(pos_tags), EMBED_DIM), dtype=np.float64)\n",
    "    weights = np.zeros((len(pos_tags),), dtype=np.float64)\n",
    "    i = 0\n",
    "    for word, tag in pos_tags:\n",
    "        vectors[i] = get_word2vec(word)\n",
    "        weights[i] = tag_w[tag] + get_tfidf(doc, word) # To be tuned\n",
    "        i += 1\n",
    "        \n",
    "    s = softmax(weights)\n",
    "    return np.dot(s, vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel_doc = docs[0]\n",
    "with open(sel_doc) as fp:\n",
    "    text = fp.read()\n",
    "    text = re.sub('\\n', ' ', text)\n",
    "    v = embed_doc(sel_doc, text)\n",
    "v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keeping Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(a, b):\n",
    "    na = np.linalg.norm(a)\n",
    "    nb =np.linalg.norm(b) \n",
    "    return np.dot(a, b) / (na * nb) if na > 0 and nb > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_query(query):\n",
    "    pos_tags = pos_tag(query)\n",
    "    vectors = np.zeros((len(pos_tags), EMBED_DIM), dtype=np.float64)\n",
    "    weights = np.zeros((len(pos_tags),), dtype=np.float64)\n",
    "    i = 0\n",
    "    for word, tag in pos_tags:\n",
    "        vectors[i] = get_word2vec(word)\n",
    "        weights[i] = 1 / len(pos_tags)\n",
    "        i += 1\n",
    "    return np.dot(weights, vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'هوش مصنوعی  تاریخچه'\n",
    "\n",
    "q_vec = embed_query(query)\n",
    "q_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'relevant': [688],\n",
       " 'similar_high': [689, 690, 691, 692, 693],\n",
       " 'similar_low': [704, 705, 706, 707, 708, 112, 709, 710, 711, 712],\n",
       " 'similar_med': [694, 695, 696, 697, 698, 699, 700, 701, 702, 703]}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dev[query]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1.])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1, 2, 3])\n",
    "b = np.array([1, 2, 3])\n",
    "a /b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3258, 1)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(doc_vectors, axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error ../dataset/IR_dataset/2885.txt at 569 occurred.\n"
     ]
    }
   ],
   "source": [
    "doc_vectors = np.zeros((len(docs), EMBED_DIM), dtype=np.float64)\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    with open(doc) as fp:\n",
    "        text = fp.read()\n",
    "        text = re.sub('\\n', ' ', text)\n",
    "    try:\n",
    "        doc_vectors[i] = embed_doc(doc, text)\n",
    "    except:\n",
    "        print('Error', doc, 'at', i, 'occurred.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-86-1d7a7f331b3b>:2: RuntimeWarning: invalid value encountered in true_divide\n",
      "  np.dot(doc_vectors, q_vec) / (np.linalg.norm(q_vec) * np.linalg.norm(doc_vectors, axis=1)))),\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(693, 0.7732056293722734),\n",
       " (2964, 0.6861036261664982),\n",
       " (2667, 0.6794550296090047),\n",
       " (225, 0.6743780133711479),\n",
       " (1971, 0.6712737159226223),\n",
       " (702, 0.6710633135617474),\n",
       " (496, 0.6536748979732013),\n",
       " (2965, 0.6532094453207982),\n",
       " (698, 0.6475874718421846),\n",
       " (1705, 0.647277432573165),\n",
       " (48, 0.6400741734479827),\n",
       " (3256, 0.6382923131028083),\n",
       " (2643, 0.6379608371356469),\n",
       " (3151, 0.6378615745589842),\n",
       " (2524, 0.6325519425081119),\n",
       " (1953, 0.6322054717449795),\n",
       " (104, 0.6315802982631057),\n",
       " (3236, 0.6286623733277574),\n",
       " (3055, 0.6284414570809459),\n",
       " (360, 0.6251452987920091)]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(list(zip([int(d.split('/')[-1].split('.')[0]) for d in docs], \n",
    "                np.dot(doc_vectors, q_vec) / (np.linalg.norm(q_vec) * np.linalg.norm(doc_vectors, axis=1)))), \n",
    "                key=lambda x: -x[1])[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(100, 4.88990204693272),\n",
       " (693, 4.8091458808698775),\n",
       " (690, 4.774438527249401),\n",
       " (98, 4.43592442748941),\n",
       " (319, 4.356160218514877),\n",
       " (97, 4.347208878282771),\n",
       " (2667, 4.282882331320097),\n",
       " (689, 4.25378830226043),\n",
       " (320, 4.160616948221811),\n",
       " (2964, 4.154929720172538),\n",
       " (323, 4.152286463288714),\n",
       " (691, 4.148186248003334),\n",
       " (852, 4.143938147481116),\n",
       " (701, 4.139121811975143),\n",
       " (700, 4.122063291218833),\n",
       " (688, 4.102835803235924),\n",
       " (692, 4.102442926602915),\n",
       " (99, 4.094033596083273),\n",
       " (1631, 4.09306477797655),\n",
       " (3030, 4.088330743742448)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(list(zip([int(d.split('/')[-1].split('.')[0]) for d in docs], np.dot(doc_vectors, q_vec))), \n",
    "       key=lambda x: -x[1])[:20]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
