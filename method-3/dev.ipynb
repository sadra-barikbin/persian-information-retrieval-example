{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "with open('docs.txt') as fp:\n",
    "    docs = [d.strip() for d in fp.readlines()]\n",
    "docs_r = {k:i for i, k in enumerate(docs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../dataset/IR_dataset/2048.txt',\n",
       " '../dataset/IR_dataset/2404.txt',\n",
       " '../dataset/IR_dataset/661.txt',\n",
       " '../dataset/IR_dataset/1252.txt',\n",
       " '../dataset/IR_dataset/726.txt',\n",
       " '../dataset/IR_dataset/3029.txt',\n",
       " '../dataset/IR_dataset/329.txt',\n",
       " '../dataset/IR_dataset/1481.txt',\n",
       " '../dataset/IR_dataset/1511.txt',\n",
       " '../dataset/IR_dataset/1127.txt',\n",
       " '../dataset/IR_dataset/2447.txt',\n",
       " '../dataset/IR_dataset/2907.txt',\n",
       " '../dataset/IR_dataset/630.txt',\n",
       " '../dataset/IR_dataset/3008.txt',\n",
       " '../dataset/IR_dataset/494.txt',\n",
       " '../dataset/IR_dataset/321.txt',\n",
       " '../dataset/IR_dataset/40.txt',\n",
       " '../dataset/IR_dataset/3131.txt',\n",
       " '../dataset/IR_dataset/2192.txt',\n",
       " '../dataset/IR_dataset/2167.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dev Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../dataset/evaluation_IR.yml', 'r') as f:\n",
    "    dataset = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dev = {k:v for k, v in list(dataset.items())[-30:]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['نظریه اصل موضوعی مجموعه\\u200cها  سازگاری و عدم وابستگی در ZFC',\n",
       " 'نظریه نسبیت  نسبیت عام',\n",
       " 'نلسون ماندلا  آغاز',\n",
       " 'نهنگ قاتل  هوش',\n",
       " 'نوا (دستگاه موسیقی)  گوشه\\u200cها',\n",
       " 'نیروهای محور  اتحاد دانوب، اختلاف بر سر اتریش',\n",
       " 'هاینریش هیملر  استحکام قدرت',\n",
       " 'هاینریش هیملر  رابطه با هیتلر',\n",
       " 'هسته لینوکس',\n",
       " 'هسته لینوکس  درگیری\\u200cهای جامعه توسعه',\n",
       " 'هسته لینوکس  مدل توسعه',\n",
       " 'هم\\u200cارزی جرم و انرژی  کاربست\\u200cپذیری فرمول',\n",
       " 'هندسه جبری',\n",
       " 'هوش مصنوعی  تاریخچه',\n",
       " 'واپاشی هسته\\u200cای  پایداری و ناپایداری ایزوتوپ\\u200cها',\n",
       " 'وشمگیر  وضعیت سیاسی-اجتماعی قرن چهارم هجری',\n",
       " 'ولایت قندهار  تمدن مندیگک',\n",
       " 'ولفگانگ آمادئوس موتسارت  موتسارت در وین',\n",
       " 'ونکوور  سیستم حمل و نقل شهری',\n",
       " 'ونکوور  معماری',\n",
       " 'پروین اعتصامی',\n",
       " 'پرچم ایران  پیش از پادشاهی پهلوی\\u200cها',\n",
       " 'پیمان کیوتو  اتحادیه اروپا',\n",
       " 'چرخه آب  توصیف',\n",
       " 'چنگیز خان  کودکی',\n",
       " 'چهاردهمین دالایی لاما  اوان زندگی و سابقه',\n",
       " 'کارل مارکس  اقتصاد، تاریخ و جامعه',\n",
       " 'گرجستان  تاریخ',\n",
       " 'یانی  تأثیرپذیری\\u200cهای موسیقایی',\n",
       " 'یونسکو  فعالیت\\u200cها']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dataset_dev.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-28T18:07:58.280925Z",
     "iopub.status.busy": "2021-12-28T18:07:58.280313Z",
     "iopub.status.idle": "2021-12-28T18:08:03.762250Z",
     "shell.execute_reply": "2021-12-28T18:08:03.761641Z",
     "shell.execute_reply.started": "2021-12-28T18:07:58.280851Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-12-28T18:08:03.765429Z",
     "iopub.status.busy": "2021-12-28T18:08:03.765188Z",
     "iopub.status.idle": "2021-12-28T18:08:03.830735Z",
     "shell.execute_reply": "2021-12-28T18:08:03.829874Z",
     "shell.execute_reply.started": "2021-12-28T18:08:03.765404Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open('pos/words.txt') as fp:\n",
    "    words_all = [line.strip() for line in fp.readlines()]\n",
    "word2int = {k:i for i, k in enumerate(words_all)}\n",
    "VOCAB_SIZE = len(word2int) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-12-28T18:08:03.831894Z",
     "iopub.status.busy": "2021-12-28T18:08:03.831690Z",
     "iopub.status.idle": "2021-12-28T18:08:03.837395Z",
     "shell.execute_reply": "2021-12-28T18:08:03.836288Z",
     "shell.execute_reply.started": "2021-12-28T18:08:03.831870Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open('pos/tags.txt') as fp:\n",
    "    tags_all = [line.strip() for line in fp.readlines()]\n",
    "tag2int = {k:i for i, k in enumerate(tags_all)}\n",
    "TAGS_NO = len(tag2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-28T18:08:03.839099Z",
     "iopub.status.busy": "2021-12-28T18:08:03.838562Z",
     "iopub.status.idle": "2021-12-28T18:08:05.982290Z",
     "shell.execute_reply": "2021-12-28T18:08:05.980065Z",
     "shell.execute_reply.started": "2021-12-28T18:08:03.839054Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "pos_tagger = load_model('models/pos_lstm_3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-12-28T18:08:06.451257Z",
     "iopub.status.busy": "2021-12-28T18:08:06.451020Z",
     "iopub.status.idle": "2021-12-28T18:08:06.460204Z",
     "shell.execute_reply": "2021-12-28T18:08:06.459267Z",
     "shell.execute_reply.started": "2021-12-28T18:08:06.451234Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pos_tagger.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "SEQ_LEN = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_chars = [\n",
    "    'آ', 'أ', 'ؤ', 'إ', 'ئ', 'ا', 'ب', 'ة', 'ت', 'ث', 'ج', 'ح', 'خ', 'د', 'ذ', 'ر', 'ز', 'س', 'ش',\n",
    "    'ص','ض','ط','ظ','ع','غ','ف','ق','ك','ل','م','ن','ه','و','ى','ي','٠','١','٢','٣', '٤', '٥', '٦', '٧',\n",
    "    '٨', '٩', 'چ', 'ژ', 'ک', 'گ', 'ھ', 'ی', '۰', '۱', '۲', '۳', '۴', '۵', '۶', '۷', '۸', '۹',\n",
    "    '\\u200c', '\\u200d', '\\u200e', '\\u200f',\n",
    "    'ﭼ', 'ﯽ', 'ﯾ', 'ﯿ', 'ﷲ', 'ﺀ', 'ﺄ', 'ﺆ', 'ﺋ', 'ﺎ', 'ﺑ', 'ﺔ', 'ﺗ', 'ﺘ', 'ﺧ', 'ﺪ', 'ﺮ', 'ﺳ', 'ﺴ', 'ﺿ',\n",
    "    'ﻋ','ﻌ', 'ﻗ', 'ﻠ', 'ﻣ', 'ﻨ', 'ﻼ', '￼', 'پ',]\n",
    "\n",
    "trans_chars = [\n",
    "    'ً', 'ٌ', 'ٍ', 'َ', 'ُ', 'ِ', 'ّ', 'ْ', 'ٓ', 'ٔ',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "def normalize_text(text):\n",
    "    text = re.sub('[' + ''.join(trans_chars) + ']', '', text)\n",
    "    text = re.sub('[^' + ''.join(allowed_chars) + ']', ' ', text)\n",
    "#     text = re.sub('ئ', 'ی', text)\n",
    "    text = re.sub('ء', '', text)\n",
    "    text = re.sub('[\\s]+', ' ', text)\n",
    "    return text\n",
    "\n",
    "def encode_text(text):\n",
    "    text = normalize_text(text)\n",
    "    tokens = nltk.tokenize.word_tokenize(text)\n",
    "    return tokens, [word2int[word] if word in word2int else word2int['[UNK]'] for word in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "def pos_tag(text):\n",
    "    test_X = []\n",
    "    test_words = []\n",
    "\n",
    "    text_words, text_tokens = encode_text(text)\n",
    "    words_list = list(chunks(text_words, SEQ_LEN))\n",
    "    tokens_list = list(chunks(text_tokens, SEQ_LEN))\n",
    "    test_X += tokens_list\n",
    "    test_words += words_list\n",
    "    test_X = pad_sequences(test_X, maxlen=SEQ_LEN, padding='post')\n",
    "    pred_outs = pos_tagger.predict(test_X)\n",
    "    pred_args = np.argmax(pred_outs, axis=2)\n",
    "    pred_tags = []\n",
    "    for i, pred in enumerate(pred_args):\n",
    "        cur_tags = [tags_all[i] if i in range(len(tags_all)) else 'UNK' for i in pred]\n",
    "        cur_pairs = list(zip(test_words[i], cur_tags))\n",
    "        pred_tags += cur_pairs\n",
    "    return pred_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tag Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('اصل', 'N_SING'), ('موضوع', 'N_SING'), ('اجتماع', 'N_SING'), ('بیان', 'N_SING'), ('می\\u200cکند', 'V_PRS'), ('یا', 'CON'), ('به', 'P'), ('بیان', 'N_SING'), ('دیگر', 'ADJ'), ('برای', 'P'), ('هر', 'DET'), ('دسته', 'N_SING'), ('دلخواه', 'ADJ'), ('از', 'P'), ('مجموعه\\u200cها', 'N_PL'), ('مجموعه\\u200cای', 'N_SING'), ('وجود', 'N_SING'), ('دارد', 'V_PRS'), ('که', 'CON'), ('شامل', 'ADJ'), ('همه', 'PRO'), ('عناصری', 'N_PL'), ('است', 'V_PRS'), ('که', 'CON'), ('حداقل', 'ADV'), ('به', 'P'), ('یکی', 'PRO'), ('از', 'P'), ('مجموعه\\u200cهای', 'N_PL'), ('دسته', 'N_SING'), ('مفروض', 'N_SING'), ('متعلق', 'ADJ'), ('باشند', 'V_SUB'), ('به', 'P'), ('بیان', 'N_SING'), ('دیگر', 'ADJ'), ('اگر', 'CON'), ('دسته\\u200cای', 'N_SING'), ('از', 'P'), ('مجموعه\\u200cها', 'N_PL'), ('باشد', 'V_SUB'), ('مجموعه\\u200cای', 'N_SING'), ('چون', 'CON'), ('وجود', 'N_SING'), ('دارد', 'V_PRS'), ('که', 'CON'), ('اگر', 'CON'), ('موجود', 'ADJ'), ('باشد', 'V_SUB'), ('به\\u200cطوری\\u200cکه', 'N_SING'), ('آنگاه', 'ADV'), ('اما', 'CON'), ('توجه', 'N_SING'), ('داشته', 'V_PP'), ('باشید', 'V_SUB'), ('که', 'CON'), ('مجموعه', 'N_SING'), ('فراگیر', 'ADJ'), ('که', 'CON'), ('تا', 'P'), ('به', 'P'), ('حال', 'N_SING'), ('وجود', 'N_SING'), ('آن', 'PRO'), ('را', 'CLITIC'), ('بر', 'P'), ('اساس', 'N_SING'), ('اصل', 'N_SING'), ('موضوع', 'N_SING'), ('اجتماع', 'N_SING'), ('تضمین', 'N_SING'), ('کرده\\u200cایم', 'V_PP'), ('ممکن', 'ADJ'), ('است', 'V_PRS'), ('بیش', 'ADJ'), ('از', 'P'), ('مورد', 'N_SING'), ('نیاز', 'N_SING'), ('فراگیر', 'ADJ'), ('باشد', 'V_SUB'), ('و', 'CON'), ('شامل', 'ADJ'), ('عناصری', 'N_PL'), ('باشد', 'V_SUB'), ('که', 'CON'), ('به', 'P'), ('هیچ\\u200cیک', 'PRO'), ('از', 'P'), ('عناصر', 'N_PL'), ('در', 'P'), ('دسته', 'N_SING'), ('متعلق', 'ADJ'), ('نباشند', 'V_SUB'), ('چرا', 'ADV_I'), ('که', 'CON'), ('اصل', 'N_SING'), ('موضوع', 'N_SING'), ('اجتماع', 'N_SING'), ('بیان', 'N_SING'), ('می\\u200cکند', 'V_PRS'), ('شامل', 'ADJ'), ('عناصر', 'N_PL'), ('مجموعه\\u200cهای', 'N_PL'), ('در', 'P'), ('است', 'V_PRS'), ('ولی', 'CON'), ('تضمین', 'N_SING'), ('نمی\\u200cکند', 'V_PRS'), ('که', 'CON'), ('این', 'DET'), ('مجموعه', 'N_SING'), ('شامل', 'ADJ'), ('اعضای', 'N_PL'), ('دیگری', 'ADJ'), ('نیست', 'V_PRS'), ('برای', 'P'), ('رفع', 'N_SING'), ('این', 'DET'), ('مشکل', 'N_SING'), ('و', 'CON'), ('ایجاد', 'N_SING'), ('مجموعه\\u200cای', 'N_SING'), ('که', 'CON'), ('دقیقا', 'ADV'), ('شامل', 'ADJ'), ('همه', 'PRO'), ('عناصری', 'N_PL'), ('باشد', 'V_SUB'), ('که', 'CON'), ('به', 'P'), ('حداقل', 'ADV'), ('یکی', 'PRO'), ('از', 'P'), ('مجموعه\\u200cهای', 'N_PL'), ('دسته', 'N_SING'), ('متعلق', 'ADJ'), ('باشند', 'V_SUB'), ('کافی', 'ADJ'), ('است', 'V_PRS'), ('اصل', 'N_SING'), ('موضوع', 'N_SING'), ('تصریح', 'N_SING'), ('را', 'CLITIC'), ('به', 'P'), ('کار', 'N_SING'), ('گرفته', 'V_PP'), ('و', 'CON'), ('مجموعه', 'N_SING'), ('به', 'P'), ('ازا', 'N_SING'), ('یک', 'NUM'), ('را', 'CLITIC'), ('تشکیل', 'N_SING'), ('دهیم', 'V_SUB'), ('در', 'P'), ('این', 'DET'), ('صورت', 'N_SING'), ('شرط', 'N_SING'), ('لازم', 'ADJ'), ('و', 'CON'), ('کافی', 'ADJ'), ('برای', 'P'), ('اینکه', 'N_SING'), ('ای', 'N_SING'), ('متعلق', 'ADJ'), ('به', 'P'), ('این', 'DET'), ('مجموعه', 'N_SING'), ('باشد', 'V_SUB'), ('این', 'PRO'), ('است', 'V_PRS'), ('که', 'CON'), ('ای', 'INT'), ('موجود', 'ADJ'), ('باشد', 'V_SUB'), ('که', 'CON'), ('یعنی', 'CON'), ('به', 'P'), ('حداقل', 'ADV'), ('یکی', 'PRO'), ('از', 'P'), ('مجموعه\\u200cهای', 'N_PL'), ('دسته', 'N_SING'), ('متعلق', 'ADJ'), ('باشد', 'V_SUB'), ('به', 'P'), ('زبان', 'N_SING'), ('منطق', 'N_SING'), ('ریاضی', 'N_SING'), ('داریم', 'V_PRS'), ('اصل', 'N_SING'), ('موضوع', 'N_SING'), ('گسترش', 'N_SING'), ('یگانگی', 'N_SING'), ('این', 'DET'), ('مجموعه', 'N_SING'), ('را', 'CLITIC'), ('تضمین', 'N_SING'), ('می\\u200cکند', 'V_PRS'), ('و', 'CON'), ('لذا', 'CON'), ('می\\u200cتوان', 'V_AUX'), ('برای', 'P'), ('آن', 'PRO'), ('نام', 'N_SING'), ('و', 'CON'), ('نماد', 'N_SING'), ('مخصوصی', 'ADJ'), ('را', 'CLITIC'), ('اختصاص', 'N_SING'), ('داد', 'V_PA'), ('این', 'DET'), ('مجموعه', 'N_SING'), ('را', 'CLITIC'), ('اجتماع', 'N_SING'), ('دسته', 'N_SING'), ('از', 'P'), ('مجموعه\\u200cها', 'N_PL'), ('می\\u200cخوانیم', 'V_PRS'), ('و', 'CON'), ('با', 'P'), ('نمادهای', 'N_PL'), ('و', 'CON'), ('نمایش', 'N_SING'), ('می\\u200cدهیم', 'V_PRS'), ('و', 'CON'), ('مطابق', 'ADJ'), ('تعریف', 'N_SING'), ('اگر', 'CON'), ('دسته', 'N_SING'), ('از', 'P'), ('مجموعه\\u200cها', 'N_PL'), ('تهی', 'ADJ'), ('باشد', 'V_SUB'), ('در', 'P'), ('این', 'DET'), ('صورت', 'N_SING'), ('مطابق', 'ADJ'), ('تعریف', 'N_SING'), ('اجتماع', 'N_SING'), ('آن', 'PRO'), ('نیز', 'CON'), ('تهی', 'ADJ'), ('خواهد', 'V_AUX'), ('بود', 'V_PA'), ('پس', 'ADV'), ('اجتماع', 'N_SING'), ('دسته\\u200cای', 'N_SING'), ('تهی', 'ADJ'), ('از', 'P'), ('مجموعه\\u200cها', 'N_PL'), ('تهی', 'ADJ'), ('است', 'V_PRS'), ('همچنین', 'CON'), ('اگر', 'CON'), ('و', 'CON'), ('دو', 'NUM'), ('مجموعه', 'N_SING'), ('باشند', 'V_SUB'), ('دسته', 'N_SING'), ('بنابر', 'N_SING'), ('اصل', 'N_SING'), ('موضوع', 'N_SING'), ('زوج', 'N_SING'), ('سازی', 'ADJ'), ('یک', 'NUM'), ('مجموعه', 'N_SING'), ('است', 'V_PRS'), ('و', 'CON'), ('نیز', 'CON'), ('بنابر', 'N_SING'), ('اصل', 'N_SING'), ('موضوع', 'N_SING'), ('اجتماع', 'N_SING'), ('مجموعه', 'N_SING'), ('شامل', 'ADJ'), ('همه', 'PRO'), ('عناصری', 'N_PL'), ('که', 'CON'), ('به', 'P'), ('حداقل', 'ADV'), ('یکی', 'PRO'), ('از', 'P'), ('مجموعه\\u200cهای', 'N_PL'), ('متعلق', 'ADJ'), ('می\\u200cباشند', 'V_PRS'), ('وجود', 'N_SING'), ('دارد', 'V_PRS'), ('در', 'P'), ('این', 'DET'), ('صورت', 'N_SING'), ('اجتماع', 'N_SING'), ('دسته', 'N_SING'), ('را', 'CLITIC'), ('اجتماع', 'N_SING'), ('دو', 'NUM'), ('مجموعه', 'N_SING'), ('و', 'CON'), ('می\\u200cگوییم', 'V_PRS'), ('و', 'CON'), ('آن', 'PRO'), ('را', 'CLITIC'), ('به', 'P'), ('صورت', 'N_SING'), ('نشان', 'N_SING'), ('می\\u200cدهیم', 'V_PRS'), ('و', 'CON'), ('داریم', 'V_PRS'), ('پس', 'ADV'), ('بنا', 'P'), ('به', 'P'), ('تعریف', 'N_SING'), ('و', 'CON'), ('لذا', 'CON'), ('داریم', 'V_PRS'), ('با', 'P'), ('توجه', 'N_SING'), ('به', 'P'), ('تعاریف', 'N_PL'), ('فوق', 'ADJ'), ('روابط', 'N_PL'), ('زیر', 'P'), ('را', 'CLITIC'), ('داریم', 'V_PRS'), ('اثبات', 'N_SING'), ('این', 'DET'), ('روابط', 'N_PL'), ('با', 'P'), ('استفاده', 'N_SING'), ('از', 'P'), ('تعاریف', 'N_PL'), ('ساده', 'ADJ'), ('است', 'V_PRS'), ('البته', 'ADV'), ('بر', 'P'), ('هر', 'DET'), ('علاقه\\u200cمند', 'N_SING'), ('ریاضی', 'N_SING'), ('واجب', 'ADJ'), ('است', 'V_PRS'), ('که', 'CON'), ('حداقل', 'ADV'), ('یکبار', 'N_SING'), ('آن\\u200cها', 'PRO'), ('را', 'CLITIC'), ('اثبات', 'N_SING'), ('کند', 'V_SUB'), ('اجتماع', 'N_SING'), ('خاصیت', 'N_SING'), ('جابجایی', 'N_SING'), ('دارد', 'V_PRS'), ('اجتماع', 'N_SING'), ('خاصیت', 'N_SING'), ('شرکت\\u200cپذیری', 'N_SING'), ('دارد', 'V_PRS'), ('اجتماع', 'N_SING'), ('خاصیت', 'N_SING'), ('خود', 'PRO'), ('توانی', 'N_SING'), ('دارد', 'V_PRS'), ('اگر', 'CON'), ('و', 'CON'), ('فقط', 'ADV'), ('اگر', 'CON')]\n"
     ]
    }
   ],
   "source": [
    "from random import sample\n",
    "\n",
    "with open('../dataset/IR_dataset/78.txt') as fp:\n",
    "    text = fp.read()\n",
    "    text = re.sub('\\n', ' ', text)\n",
    "    print(pos_tag(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "word2vec = np.load(\"w2v/word2vec2.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('w2v/vocab2.txt') as fp:\n",
    "    vocab = [l.strip() for l in fp.readlines()]\n",
    "vocab_r = {k:i for i, k in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75011, (75011, 200))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab), word2vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word2vec(word):\n",
    "    return word2vec[vocab_r[word]] if word in vocab_r else np.zeros((EMBED_DIM,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tag Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_w = {\n",
    "    'PAD': 0,\n",
    "    'ADJ': 1,\n",
    "    'ADJ_CMPR': 1,\n",
    "    'ADJ_INO': 1,\n",
    "    'ADJ_SUP': 1,\n",
    "    'ADJ_VOC': 1,\n",
    "    'ADV': 0.5,\n",
    "    'ADV_COMP': 0.5,\n",
    "    'ADV_I': 0.5,\n",
    "    'ADV_LOC': 0.5,\n",
    "    'ADV_NEG': 0.5,\n",
    "    'ADV_TIME': 0.5,\n",
    "    'CLITIC': 0,\n",
    "    'CON': 0,\n",
    "    'DELM': 0,\n",
    "    'DET': 0,\n",
    "    'FW': 2,\n",
    "    'INT': 0,\n",
    "    'NUM': 2,\n",
    "    'N_PL': 12,\n",
    "    'N_SING': 12,\n",
    "    'N_VOC': 6,\n",
    "    'P': 0,\n",
    "    'PREV': 0,\n",
    "    'PRO': 0,\n",
    "    'SYM': 0,\n",
    "    'UNK': 0,\n",
    "    'V_AUX': 0,\n",
    "    'V_IMP': 0,\n",
    "    'V_PA': 0,\n",
    "    'V_PP': 0,\n",
    "    'V_PRS': 0,\n",
    "    'V_SUB': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-Idf Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import load_npz\n",
    "\n",
    "tfidf = load_npz('tfidf/tfidf.npz')\n",
    "\n",
    "tfidf_words = []\n",
    "with open('tfidf/words.txt') as fp:\n",
    "    tfidf_words = [w.strip() for w in fp.readlines()]\n",
    "tfidf_words_r = {k:i for i, k in enumerate(tfidf_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30680919856295225"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf[docs_r['../dataset/IR_dataset/2048.txt'], tfidf_words_r['پودر']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf(doc, word):\n",
    "    return tfidf[docs_r[doc], tfidf_words_r[word]] if word in tfidf_words_r else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_doc(doc, text):\n",
    "    pos_tags = pos_tag(text)\n",
    "    vectors = np.zeros((len(pos_tags), EMBED_DIM), dtype=np.float64)\n",
    "    weights = np.zeros((len(pos_tags),), dtype=np.float64)\n",
    "    i = 0\n",
    "    for word, tag in pos_tags:\n",
    "        vectors[i] = get_word2vec(word)\n",
    "        weights[i] = tag_w[tag] + get_tfidf(doc, word) # To be tuned\n",
    "        i += 1\n",
    "        \n",
    "    s = softmax(weights)\n",
    "    return np.dot(s, vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel_doc = docs[0]\n",
    "with open(sel_doc) as fp:\n",
    "    text = fp.read()\n",
    "    text = re.sub('\\n', ' ', text)\n",
    "    v = embed_doc(sel_doc, text)\n",
    "v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keeping Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(a, b):\n",
    "    na = np.linalg.norm(a)\n",
    "    nb =np.linalg.norm(b) \n",
    "    return np.dot(a, b) / (na * nb) if na > 0 and nb > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_query(query):\n",
    "    pos_tags = pos_tag(query)\n",
    "    vectors = np.zeros((len(pos_tags), EMBED_DIM), dtype=np.float64)\n",
    "    weights = np.zeros((len(pos_tags),), dtype=np.float64)\n",
    "    i = 0\n",
    "    for word, tag in pos_tags:\n",
    "        vectors[i] = get_word2vec(word)\n",
    "        weights[i] = 1 / len(pos_tags)\n",
    "        i += 1\n",
    "    return np.dot(weights, vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'هوش مصنوعی  تاریخچه'\n",
    "\n",
    "q_vec = embed_query(query)\n",
    "q_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'relevant': [2712], 'similar_high': [2713, 2714, 661], 'similar_low': [2724, 2725, 2726, 2727, 2728, 2729, 2463, 2730, 2731, 2732], 'similar_med': [2715, 2716, 2717, 1215, 2718, 2719, 2720, 2721, 2722, 2723]}\n",
      "{'relevant': [1618], 'similar_high': [1617, 1619, 1620, 1621], 'similar_low': [2562, 1056, 2563, 2564, 2565, 2566, 2567, 2132, 2568, 2569], 'similar_med': [2554, 2555, 1627, 2556, 1887, 2557, 2558, 2559, 2560, 2561]}\n",
      "{'relevant': [2135], 'similar_high': [1383, 2136, 2137], 'similar_low': [1140, 2140, 2141, 2142, 2143, 2144, 2145, 2146, 2147, 2148], 'similar_med': [2138, 2139]}\n",
      "{'relevant': [3236], 'similar_high': [3237, 2109, 3238, 3239], 'similar_low': [3248, 3249, 3250, 3251, 3252, 3253, 3254, 3255, 3256, 3257], 'similar_med': [3240, 3241, 2850, 3242, 2839, 3243, 3244, 3245, 3246, 3247]}\n",
      "{'relevant': [1979], 'similar_high': [727, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 953], 'similar_low': [1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005], 'similar_med': [957, 1988, 950, 1989, 1990, 1991, 1992, 1993, 1994, 1995]}\n",
      "{'relevant': [438], 'similar_high': [439, 440, 441, 350], 'similar_low': [451, 452, 453, 454, 455, 456, 457, 458, 459, 460], 'similar_med': [442, 443, 444, 445, 347, 446, 447, 448, 449, 450]}\n",
      "{'relevant': [450], 'similar_high': [348, 461, 443], 'similar_low': [487, 488, 489, 490, 491, 492, 493, 494, 495, 496], 'similar_med': [480, 481, 482, 463, 483, 484, 485, 486, 464, 345]}\n",
      "{'relevant': [443], 'similar_high': [450, 348, 461, 345], 'similar_low': [470, 471, 472, 473, 474, 475, 476, 477, 478, 479], 'similar_med': [462, 220, 463, 464, 465, 466, 467, 347, 468, 469]}\n",
      "{'relevant': [1076], 'similar_high': [1113, 1075, 1114, 1074, 1115, 1073], 'similar_low': [1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133], 'similar_med': [1116, 1117, 1079, 1118, 1119, 1120, 1121, 1122, 1123, 1086]}\n",
      "{'relevant': [1073], 'similar_high': [1074, 1075, 1076], 'similar_low': [1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096], 'similar_med': [1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086]}\n",
      "{'relevant': [1075], 'similar_high': [1074, 1076, 1073], 'similar_low': [1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112], 'similar_med': [1097, 1098, 1099, 1086, 1080, 1100, 1082, 1101, 1102, 1079]}\n",
      "{'relevant': [1768], 'similar_high': [2515, 1759, 2514], 'similar_low': [2576, 2577, 2578, 2579, 2580, 2581, 2582, 2583, 2584, 2585], 'similar_med': [1620, 2570, 2571, 580, 2572, 2573, 2574, 1763, 1444, 2575]}\n",
      "{'relevant': [1208], 'similar_high': [1209, 1210, 1211], 'similar_low': [1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230], 'similar_med': [1212, 1213, 1214, 1215, 1216, 268, 1217, 1218, 1219, 1220]}\n",
      "{'relevant': [688], 'similar_high': [689, 690, 691, 692, 693], 'similar_low': [704, 705, 706, 707, 708, 112, 709, 710, 711, 712], 'similar_med': [694, 695, 696, 697, 698, 699, 700, 701, 702, 703]}\n",
      "{'relevant': [71], 'similar_high': [72, 73, 74, 75, 76], 'similar_low': [87, 88, 89, 90, 91, 92, 93, 94, 95, 96], 'similar_med': [77, 78, 79, 80, 81, 82, 83, 84, 85, 86]}\n",
      "{'relevant': [3119], 'similar_high': [3120, 3121, 3122], 'similar_low': [3132, 3133, 3134, 2722, 403, 3135, 3136, 3137, 3138, 3139], 'similar_med': [3123, 3124, 3125, 3126, 3127, 3128, 3129, 3130, 2750, 3131]}\n",
      "{'relevant': [1017], 'similar_high': [1018, 32, 1019], 'similar_low': [1022, 1023, 1024, 1025, 1026, 1027, 947, 1028, 1029, 1030], 'similar_med': [1020, 1021]}\n",
      "{'relevant': [2113], 'similar_high': [2114, 2115, 2116], 'similar_low': [2126, 2127, 2128, 2129, 2130, 1194, 2131, 2132, 2133, 2134], 'similar_med': [245, 2117, 2118, 2119, 2120, 2121, 2122, 2123, 2124, 2125]}\n",
      "{'relevant': [124], 'similar_high': [146, 125, 147, 148, 123, 129, 149, 128, 122, 134], 'similar_low': [157, 158, 159, 160, 161, 162, 163, 164, 165, 166], 'similar_med': [135, 150, 151, 152, 153, 154, 155, 130, 156, 133]}\n",
      "{'relevant': [121], 'similar_high': [122, 123, 124, 125], 'similar_low': [136, 137, 138, 139, 140, 141, 142, 143, 144, 145], 'similar_med': [126, 127, 128, 129, 130, 131, 132, 133, 134, 135]}\n",
      "{'relevant': [735], 'similar_high': [736, 737, 738], 'similar_low': [744, 745, 746, 747, 748, 749, 750, 751, 752, 753], 'similar_med': [739, 740, 741, 742, 743]}\n",
      "{'relevant': [414], 'similar_high': [415, 416, 417, 418], 'similar_low': [429, 430, 431, 432, 433, 434, 435, 293, 436, 437], 'similar_med': [419, 420, 421, 422, 423, 424, 425, 426, 427, 428]}\n",
      "{'relevant': [23], 'similar_high': [1202, 719, 721], 'similar_low': [1198, 2061, 2062, 2063, 2064, 2065, 2066, 1280, 2067, 2068], 'similar_med': [2053, 2054, 2055, 2056, 2057, 2058, 1465, 2059, 2060, 1473]}\n",
      "{'relevant': [2863], 'similar_high': [2864, 2865, 2866, 632], 'similar_low': [2876, 2877, 2878, 2879, 2880, 2881, 2882, 2883, 2884, 2885], 'similar_med': [2867, 2868, 2869, 2870, 2871, 2795, 2872, 2873, 2874, 2875]}\n",
      "{'relevant': [52], 'similar_high': [53, 54, 55, 56, 57, 58], 'similar_low': [61, 62, 63, 64, 65, 66, 67, 68, 69, 70], 'similar_med': [59, 60]}\n",
      "{'relevant': [2968], 'similar_high': [2969, 2970, 2971, 2972, 2973], 'similar_low': [2978, 2008, 2979, 2980, 2981, 344, 2982, 2983, 2984, 2985], 'similar_med': [2974, 2975, 2976, 2977, 2091]}\n",
      "{'relevant': [365], 'similar_high': [366, 367, 368], 'similar_low': [379, 380, 381, 49, 382, 383, 384, 385, 386, 387], 'similar_med': [369, 370, 371, 372, 373, 374, 375, 376, 377, 378]}\n",
      "{'relevant': [388], 'similar_high': [389, 390, 391, 392, 393, 394], 'similar_low': [404, 405, 406, 407, 408, 409, 410, 411, 412, 413], 'similar_med': [395, 364, 396, 397, 398, 399, 400, 401, 402, 403]}\n",
      "{'relevant': [1748], 'similar_high': [2886, 2547, 2887, 555, 2888, 2889], 'similar_low': [2897, 2898, 2899, 2900, 2901, 2902, 353, 2013, 2903, 2904], 'similar_med': [960, 2890, 2891, 2892, 2893, 2894, 2895, 546, 2125, 2896]}\n",
      "{'relevant': [1505], 'similar_high': [1506, 1507, 1508, 1509, 1510, 107, 1511, 1512, 1513, 1514], 'similar_low': [1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534], 'similar_med': [1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523, 1524]}\n"
     ]
    }
   ],
   "source": [
    "for query in dataset_dev:\n",
    "    print(dataset_dev[query])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error ../dataset/IR_dataset/2885.txt at 569 occurred.\n"
     ]
    }
   ],
   "source": [
    "doc_vectors = np.zeros((len(docs), EMBED_DIM), dtype=np.float64)\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    with open(doc) as fp:\n",
    "        text = fp.read()\n",
    "        text = re.sub('\\n', ' ', text)\n",
    "    try:\n",
    "        doc_vectors[i] = embed_doc(doc, text)\n",
    "    except:\n",
    "        print('Error', doc, 'at', i, 'occurred.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-86-1d7a7f331b3b>:2: RuntimeWarning: invalid value encountered in true_divide\n",
      "  np.dot(doc_vectors, q_vec) / (np.linalg.norm(q_vec) * np.linalg.norm(doc_vectors, axis=1)))),\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(693, 0.7732056293722734),\n",
       " (2964, 0.6861036261664982),\n",
       " (2667, 0.6794550296090047),\n",
       " (225, 0.6743780133711479),\n",
       " (1971, 0.6712737159226223),\n",
       " (702, 0.6710633135617474),\n",
       " (496, 0.6536748979732013),\n",
       " (2965, 0.6532094453207982),\n",
       " (698, 0.6475874718421846),\n",
       " (1705, 0.647277432573165),\n",
       " (48, 0.6400741734479827),\n",
       " (3256, 0.6382923131028083),\n",
       " (2643, 0.6379608371356469),\n",
       " (3151, 0.6378615745589842),\n",
       " (2524, 0.6325519425081119),\n",
       " (1953, 0.6322054717449795),\n",
       " (104, 0.6315802982631057),\n",
       " (3236, 0.6286623733277574),\n",
       " (3055, 0.6284414570809459),\n",
       " (360, 0.6251452987920091)]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(list(zip([int(d.split('/')[-1].split('.')[0]) for d in docs], \n",
    "                np.dot(doc_vectors, q_vec) / (np.linalg.norm(q_vec) * np.linalg.norm(doc_vectors, axis=1)))), \n",
    "                key=lambda x: -x[1])[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(100, 4.88990204693272),\n",
       " (693, 4.8091458808698775),\n",
       " (690, 4.774438527249401),\n",
       " (98, 4.43592442748941),\n",
       " (319, 4.356160218514877),\n",
       " (97, 4.347208878282771),\n",
       " (2667, 4.282882331320097),\n",
       " (689, 4.25378830226043),\n",
       " (320, 4.160616948221811),\n",
       " (2964, 4.154929720172538),\n",
       " (323, 4.152286463288714),\n",
       " (691, 4.148186248003334),\n",
       " (852, 4.143938147481116),\n",
       " (701, 4.139121811975143),\n",
       " (700, 4.122063291218833),\n",
       " (688, 4.102835803235924),\n",
       " (692, 4.102442926602915),\n",
       " (99, 4.094033596083273),\n",
       " (1631, 4.09306477797655),\n",
       " (3030, 4.088330743742448)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(list(zip([int(d.split('/')[-1].split('.')[0]) for d in docs], np.dot(doc_vectors, q_vec))), \n",
    "       key=lambda x: -x[1])[:20]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
