{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl\">\n",
    "<h1>تولید کلمات کلیدی</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl\"> \n",
    "    در این بخش به پیاده‌سازی کامل برنامه استخراج کلمات کلیدی از متن می‌پردازیم. <br>\n",
    "    روند استخراج کلمات کلیدی را به صورت کلی بیان می‌کنیم: <br>\n",
    "    <ol>\n",
    "        <li>\n",
    "            ابتدا متن ورودی نرمال سازی و توکن‌سازی شده و سایر کارهای مربوط به پیش‌پردازش اولیه روی آن انجام می‌شود.\n",
    "        </li>\n",
    "        <li>\n",
    "            سپس متن کدگذاری شده و به برچسب‌گذار ادات سخن که در فایل دیگری از این پروژه آموزش داده شده‌است داده می‌شود و بر روی کلمات ورودی آن برچسب مربوط به نقش آن‌ها گذاشته می‌شود.\n",
    "        </li>\n",
    "        <li>\n",
    "            سپس تکه (chunk) عبارت‌های معنی دار مانند عبارت‌های اسمی، فعلی و صفتی در دنباله کلمات ورودی با توجه به ادات سخنی که در بخش قبل به دست آمده‌است مشخص می‌شوند.\n",
    "        </li>\n",
    "        <li>\n",
    "            اکنون دنباله‌هایی که حاوی عبارت‌های اسمی (زیرا کلمات کلیدی معمولاً اسم می‌باشند) هستند را میان تکه‌های به دست آمده جدا می‌کنیم. \n",
    "        </li>\n",
    "        <li>\n",
    "           سپس بر روی آن‌ها BPE را اجرا می‌کنیم و در صورتی که توسط این الگوریتم کلمه شکسته شد به این معناست که زیرکلمه‌ای از آن به شکل شایعی در کلمات کتاب آمده‌است و این به این معناست که احتمالاً کلمه کلیدی نیست. \n",
    "        </li>\n",
    "        <li>\n",
    "            و در نهایت کلمات شکسته نشده را فیلتر کرده و در صورتی که جزو کلمات نادرتر تشخیص داده شده نبودند به عنوان خروجی در نظر می‌گیریم.\n",
    "        </li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl\"> \n",
    "    <h2>بارگیری فایل‌های پیش‌پردازش و آماده‌سازی شده</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('analysis.json') as fp:\n",
    "    analysis_data = json.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl\"> \n",
    "    <h3> بارگیری برچسب‌گذار ادات سخن</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open('pos/words.txt') as fp:\n",
    "    words_all = [line.strip() for line in fp.readlines()]\n",
    "word2int = {k:i for i, k in enumerate(words_all)}\n",
    "VOCAB_SIZE = len(word2int) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open('pos/tags.txt') as fp:\n",
    "    tags_all = [line.strip() for line in fp.readlines()]\n",
    "tag2int = {k:i for i, k in enumerate(tags_all)}\n",
    "TAGS_NO = len(tag2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "pos_tagger = load_model('models/pos_lstm_3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 50)]              0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 50, 100)           7840700   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 50, 160)          115840    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 50, 34)           5474      \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,962,014\n",
      "Trainable params: 7,962,014\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "pos_tagger.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl\"> \n",
    "    <h3> بارگیری تکه‌عبارت یاب (Phrase Chunker)</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('chunker.pkl', 'rb') as fp:\n",
    "    chunk_parser = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl\"> \n",
    "    <h3> بارگیری  BPE Encoder </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bpe-encoder.pkl', 'rb') as fp:\n",
    "    bpe_encoder = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl\"> \n",
    "    <h3> بارگیری کلمات نادرتر </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('uncommon_words.txt') as fp:\n",
    "    uncommon_words = set([line.strip() for line in fp.readlines()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl\"> \n",
    "    <h2> پیش‌پردازش (نرمال‌سازی و توکن‌سازی) متن ورودی </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def encode_text(text):\n",
    "    text = re.sub('[' + analysis_data['pun'] + '*]', '', text)\n",
    "    text = re.sub('[!]', ' ', text)\n",
    "    text = re.sub('ئ', 'ی', text)\n",
    "    text = re.sub('ء', '', text)\n",
    "    text = re.sub('[\\s]+', ' ', text)\n",
    "    tokens = nltk.tokenize.word_tokenize(text)\n",
    "    return tokens, [word2int[word] if word in word2int else word2int['[UNK]'] for word in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl\"> \n",
    "    <h3> متن‌های ورودی</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open('kamal-al-din-sadoogh-clean.txt') as fp:\n",
    "    lines = [line.strip() for line in fp.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = lines[:10] # input texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl\"> \n",
    "    <h1> برچسب‌گذاری ادات سخن</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "SEQ_LEN = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def pos_tag(text):\n",
    "    test_X = []\n",
    "    test_words = []\n",
    "\n",
    "    text_words, text_tokens = encode_text(text)\n",
    "    words_list = list(chunks(text_words, SEQ_LEN))\n",
    "    tokens_list = list(chunks(text_tokens, SEQ_LEN))\n",
    "    test_X += tokens_list\n",
    "    test_words += words_list\n",
    "    test_X = pad_sequences(test_X, maxlen=SEQ_LEN, padding='post')\n",
    "    pred_outs = pos_tagger.predict(test_X)\n",
    "    pred_args = np.argmax(pred_outs, axis=2)\n",
    "    pred_tags = []\n",
    "    for i, pred in enumerate(pred_args):\n",
    "        cur_tags = [tags_all[i] if i in range(len(tags_all)) else 'UNK' for i in pred]\n",
    "        cur_pairs = list(zip(test_words[i], cur_tags))\n",
    "        pred_tags += cur_pairs\n",
    "    return pred_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tagged_texts = []\n",
    "for text in texts:\n",
    "    tagged_texts.append(pos_tag(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('ترجمه', 'N_SING'), ('کمال', 'N_SING'), ('الدین', 'N_SING')],\n",
       " [('مشخصات', 'N_PL'), ('کتاب', 'N_SING')],\n",
       " [('جلد', 'N_SING'), ('اول\\u200c', 'FW')],\n",
       " [('[', 'DELM'), ('مقدمه', 'N_SING'), ('مؤلف', 'DELM'), (']', 'DELM')],\n",
       " [('اشاره', 'N_SING')],\n",
       " [('بسم', 'FW'), ('الله', 'FW'), ('الرحمن', 'FW'), ('الرحیم', 'FW')],\n",
       " [('(', 'DELM'),\n",
       "  ('1', 'NUM'),\n",
       "  (')', 'DELM'),\n",
       "  ('حمد', 'N_SING'),\n",
       "  ('از', 'P'),\n",
       "  ('آن', 'PRO'),\n",
       "  ('خداوندی', 'N_SING'),\n",
       "  ('است', 'V_PRS'),\n",
       "  ('که', 'CON'),\n",
       "  ('پروردگار', 'N_SING'),\n",
       "  ('جهانیان', 'N_PL'),\n",
       "  ('است', 'V_PRS'),\n",
       "  ('،', 'DELM'),\n",
       "  ('و', 'CON'),\n",
       "  ('درود', 'N_SING'),\n",
       "  ('خداوند', 'N_SING'),\n",
       "  ('بر', 'P'),\n",
       "  ('پیامبرش', 'N_SING'),\n",
       "  ('محمد', 'N_SING'),\n",
       "  ('و', 'CON'),\n",
       "  ('خاندان', 'N_SING'),\n",
       "  ('طاهرینش', 'N_SING'),\n",
       "  ('باد', 'V_SUB'),\n",
       "  ('.', 'DELM')],\n",
       " [('حمد', 'N_SING'),\n",
       "  ('سزاوار', 'ADJ'),\n",
       "  ('خدای', 'N_SING'),\n",
       "  ('یکتای', 'N_SING'),\n",
       "  ('تنهاست', 'ADJ'),\n",
       "  ('،', 'DELM'),\n",
       "  ('بی\\u200cنیاز', 'ADJ'),\n",
       "  ('و', 'CON'),\n",
       "  ('زنده', 'ADJ'),\n",
       "  ('و', 'CON'),\n",
       "  ('توانا', 'ADJ'),\n",
       "  ('و', 'CON'),\n",
       "  ('دانا', 'ADJ'),\n",
       "  ('و', 'CON'),\n",
       "  ('حکیم', 'ADJ'),\n",
       "  ('و', 'CON'),\n",
       "  ('بلند', 'ADJ'),\n",
       "  ('مرتبه', 'N_SING'),\n",
       "  ('و', 'CON'),\n",
       "  ('عظیم', 'ADJ'),\n",
       "  ('،', 'DELM'),\n",
       "  ('کسی', 'N_SING'),\n",
       "  ('که', 'CON'),\n",
       "  ('از', 'P'),\n",
       "  ('صفات', 'N_PL'),\n",
       "  ('مخلوقین', 'ADJ'),\n",
       "  ('برتر', 'ADJ_CMPR'),\n",
       "  ('است', 'V_PRS'),\n",
       "  ('و', 'CON'),\n",
       "  ('صاحب', 'N_SING'),\n",
       "  ('جلال', 'N_SING'),\n",
       "  ('و', 'CON'),\n",
       "  ('اکرام', 'N_SING'),\n",
       "  ('و', 'CON'),\n",
       "  ('فضل', 'N_SING'),\n",
       "  ('و', 'CON'),\n",
       "  ('انعام', 'N_SING'),\n",
       "  ('و', 'CON'),\n",
       "  ('دارای', 'ADJ'),\n",
       "  ('مشیت', 'N_SING'),\n",
       "  ('نافذه', 'N_SING'),\n",
       "  ('و', 'CON'),\n",
       "  ('اراده', 'N_SING'),\n",
       "  ('کامله', 'ADJ'),\n",
       "  ('،', 'DELM'),\n",
       "  ('کسی', 'N_SING'),\n",
       "  ('که', 'CON'),\n",
       "  ('او', 'PRO'),\n",
       "  ('را', 'CLITIC'),\n",
       "  ('مثلی', 'N_SING'),\n",
       "  ('نیست', 'V_PRS'),\n",
       "  ('،', 'DELM'),\n",
       "  ('و', 'CON'),\n",
       "  ('شنوا', 'ADJ'),\n",
       "  ('و', 'CON'),\n",
       "  ('بینا', 'ADJ'),\n",
       "  ('است', 'V_PRS'),\n",
       "  ('.', 'DELM'),\n",
       "  ('دیدگان', 'N_PL'),\n",
       "  ('او', 'PRO'),\n",
       "  ('را', 'CLITIC'),\n",
       "  ('در', 'P'),\n",
       "  ('نمی\\u200cیابد', 'V_PRS'),\n",
       "  ('و', 'CON'),\n",
       "  ('او', 'PRO'),\n",
       "  ('دیدگان', 'N_PL'),\n",
       "  ('را', 'CLITIC'),\n",
       "  ('ادراک', 'N_SING'),\n",
       "  ('می\\u200cکند', 'V_PRS'),\n",
       "  ('و', 'CON'),\n",
       "  ('او', 'PRO'),\n",
       "  ('لطیف', 'ADJ'),\n",
       "  ('و', 'CON'),\n",
       "  ('خبیر', 'ADJ'),\n",
       "  ('است', 'V_PRS'),\n",
       "  ('.', 'DELM')],\n",
       " [('و', 'CON'),\n",
       "  ('شهادت', 'N_SING'),\n",
       "  ('می\\u200cدهم', 'V_PRS'),\n",
       "  ('که', 'CON'),\n",
       "  ('هیچ', 'ADV_NEG'),\n",
       "  ('معبودی', 'N_SING'),\n",
       "  ('جز', 'P'),\n",
       "  ('«', 'DELM'),\n",
       "  ('الله', 'FW'),\n",
       "  ('»', 'DELM'),\n",
       "  ('نیست', 'V_PRS'),\n",
       "  ('،', 'DELM'),\n",
       "  ('یکتاست', 'ADJ'),\n",
       "  ('و', 'CON'),\n",
       "  ('هیچ', 'ADV_NEG'),\n",
       "  ('شریکی', 'N_SING'),\n",
       "  ('ندارد', 'V_PRS'),\n",
       "  ('،', 'DELM'),\n",
       "  ('آفریننده', 'ADJ'),\n",
       "  ('و', 'CON'),\n",
       "  ('مالک', 'N_SING'),\n",
       "  ('هر', 'DET'),\n",
       "  ('چیزی', 'N_SING'),\n",
       "  ('است', 'V_PRS'),\n",
       "  ('و', 'CON'),\n",
       "  ('جاعل', 'N_SING'),\n",
       "  ('و', 'CON'),\n",
       "  ('محدث', 'N_SING'),\n",
       "  ('و', 'CON'),\n",
       "  ('پرورنده', 'N_PL'),\n",
       "  ('هر', 'DET'),\n",
       "  ('شی\\u200c', 'N_SING'),\n",
       "  ('است', 'V_PRS'),\n",
       "  ('.', 'DELM'),\n",
       "  ('او', 'PRO'),\n",
       "  ('به', 'P'),\n",
       "  ('حق', 'N_SING'),\n",
       "  ('حکم', 'N_SING'),\n",
       "  ('می\\u200cکند', 'V_PRS'),\n",
       "  ('و', 'CON'),\n",
       "  ('در', 'P'),\n",
       "  ('حکم', 'N_SING'),\n",
       "  ('عدل', 'N_SING'),\n",
       "  ('می\\u200cورزد', 'V_PRS'),\n",
       "  ('و', 'CON'),\n",
       "  ('به', 'P'),\n",
       "  ('قسط', 'N_SING'),\n",
       "  ('فرمان', 'N_SING'),\n",
       "  ('می\\u200cدهد', 'V_PRS'),\n",
       "  ('و', 'CON'),\n",
       "  ('به', 'P'),\n",
       "  ('عدل', 'N_SING'),\n",
       "  ('و', 'CON'),\n",
       "  ('احسان', 'N_SING'),\n",
       "  ('و', 'CON'),\n",
       "  ('عطا', 'N_SING'),\n",
       "  ('به', 'P'),\n",
       "  ('خویشان', 'N_PL'),\n",
       "  ('امر', 'N_SING'),\n",
       "  ('می\\u200cکند', 'V_PRS'),\n",
       "  ('و', 'CON'),\n",
       "  ('از', 'P'),\n",
       "  ('فحشا', 'N_PL'),\n",
       "  ('و', 'CON'),\n",
       "  ('زشتی', 'N_SING'),\n",
       "  ('و', 'CON'),\n",
       "  ('ستم', 'N_SING'),\n",
       "  ('باز', 'ADJ'),\n",
       "  ('می\\u200cدارد', 'V_PRS'),\n",
       "  ('.', 'DELM'),\n",
       "  ('(', 'DELM'),\n",
       "  ('2', 'NUM'),\n",
       "  (')', 'DELM'),\n",
       "  ('هیچ', 'ADV_NEG'),\n",
       "  ('کس', 'N_SING'),\n",
       "  ('را', 'CLITIC'),\n",
       "  ('به', 'P'),\n",
       "  ('عملی', 'N_SING'),\n",
       "  ('فوق', 'ADJ'),\n",
       "  ('تواناییش', 'N_SING'),\n",
       "  ('تکلیف', 'N_SING'),\n",
       "  ('نکند', 'V_SUB'),\n",
       "  ('و', 'CON'),\n",
       "  ('بیش', 'ADJ'),\n",
       "  ('از', 'P'),\n",
       "  ('طاقت', 'N_SING'),\n",
       "  ('بر', 'P')],\n",
       " [('کسی', 'N_SING'),\n",
       "  ('بار', 'N_SING'),\n",
       "  ('ننهد', 'ADJ'),\n",
       "  ('.', 'DELM'),\n",
       "  ('(', 'DELM'),\n",
       "  ('1', 'NUM'),\n",
       "  (')', 'DELM'),\n",
       "  ('حجت', 'N_SING'),\n",
       "  ('بالغه', 'N_SING'),\n",
       "  ('از', 'P'),\n",
       "  ('آن', 'PRO'),\n",
       "  ('اوست', 'V_PRS'),\n",
       "  ('و', 'CON'),\n",
       "  ('اگر', 'CON'),\n",
       "  ('بخواهد', 'V_SUB'),\n",
       "  ('همه', 'PRO'),\n",
       "  ('مردم', 'N_SING'),\n",
       "  ('را', 'CLITIC'),\n",
       "  ('هدایت', 'N_SING'),\n",
       "  ('می\\u200cکند', 'V_PRS'),\n",
       "  ('،', 'DELM'),\n",
       "  ('به', 'P'),\n",
       "  ('خانه', 'N_SING'),\n",
       "  ('سلم', 'N_SING'),\n",
       "  ('و', 'CON'),\n",
       "  ('سلامت', 'N_SING'),\n",
       "  ('فرا', 'PREV'),\n",
       "  ('می\\u200cخواند', 'V_PRS'),\n",
       "  ('و', 'CON'),\n",
       "  ('هر', 'DET'),\n",
       "  ('کس', 'N_SING'),\n",
       "  ('را', 'CLITIC'),\n",
       "  ('که', 'CON'),\n",
       "  ('بخواهد', 'V_SUB'),\n",
       "  ('به', 'P'),\n",
       "  ('راه', 'N_SING'),\n",
       "  ('راست', 'ADJ'),\n",
       "  ('هدایت', 'N_SING'),\n",
       "  ('می\\u200cنماید', 'V_PRS'),\n",
       "  ('.', 'DELM')]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl\"> \n",
    "    <h1> یافتن تکه‌عبارت‌ها (Phrase Chunking)</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phrase_chunk(tagged_text):\n",
    "    tree = chunk_parser.parse(tagged_text)\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees = []\n",
    "for tagged_text in tagged_texts:\n",
    "    trees.append(phrase_chunk(tagged_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl\"> \n",
    "    <h1> جدا کردن عبارت‌های اسمی و اسم‌های  موجود در آن‌ها</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_name_phrases(tree):\n",
    "    subtrees = [t for t in list(tree.subtrees(filter=lambda x: x.label()=='NNP')) if isinstance(t[0], list)]\n",
    "    word_sets = [set([d[0] for d in t.leaves()]) for t in subtrees]\n",
    "    if not word_sets:\n",
    "        return set()\n",
    "    union_words = set.union(*word_sets)\n",
    "    return union_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_names = []\n",
    "for tree in trees:\n",
    "    extracted_names.append(extract_name_phrases(tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl\"> \n",
    "    <h1> فیلتر کردن اسم‌های شکسته‌شده توسط BPE و اسم‌های نادرتر</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpe_uncommon_filter(words):\n",
    "    return uncommon_words.intersection(set([w for w in words if len(bpe_encoder.tokenize(w)) == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_keywords = []\n",
    "for names in extracted_names:\n",
    "    final_keywords.append(bpe_uncommon_filter(names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl\"> \n",
    "    <h1> کلمات کلیدی نهایی </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'الدین', 'کمال'},\n",
       " set(),\n",
       " set(),\n",
       " set(),\n",
       " {'اشاره'},\n",
       " set(),\n",
       " {'جهانیان', 'حمد', 'خاندان', 'درود', 'پروردگار', 'پیامبرش'},\n",
       " {'ادراک',\n",
       "  'اراده',\n",
       "  'اکرام',\n",
       "  'بلند',\n",
       "  'توانا',\n",
       "  'حمد',\n",
       "  'حکیم',\n",
       "  'دارای',\n",
       "  'دیدگان',\n",
       "  'زنده',\n",
       "  'سزاوار',\n",
       "  'صفات',\n",
       "  'فضل',\n",
       "  'مثلی',\n",
       "  'مرتبه'},\n",
       " {'احسان',\n",
       "  'تکلیف',\n",
       "  'جاعل',\n",
       "  'حکم',\n",
       "  'خویشان',\n",
       "  'زشتی',\n",
       "  'ستم',\n",
       "  'شهادت',\n",
       "  'طاقت',\n",
       "  'عدل',\n",
       "  'عطا',\n",
       "  'عملی',\n",
       "  'فوق',\n",
       "  'مالک',\n",
       "  'معبودی'},\n",
       " {'بالغه', 'راست', 'سلامت'}]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl\">\n",
    "<h1> پایان</h1>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
