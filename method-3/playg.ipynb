{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../dataset/evaluation_IR.yml', 'r') as f:\n",
    "    dataset = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['آدولف هیتلر  شکست و مرگ',\n",
       " 'آقامحمدخان قاجار',\n",
       " 'آل افراسیاب  فرمانروایان',\n",
       " 'ابوالحسن صبا  دوران جوانی',\n",
       " 'ابومسلم خراسانی  فرمانرانی بر خراسان',\n",
       " 'اتحادیه دموکرات مسیحی آلمان  پایگاه ایدئولوژیک',\n",
       " 'ارجاسپ  در شاهنامه',\n",
       " 'استان فارس  دانشگاه\\u200cها',\n",
       " 'استاندارد طلا  استاندارد طلا و رکود شدید اقتصادی جهان',\n",
       " 'اصل موضوع انتخاب  کاربرد اصل موضوع انتخاب و قضایای هم ارز آن',\n",
       " 'اظهارنامه مالیاتی',\n",
       " 'الکترون  برهم\\u200cکنش',\n",
       " 'انرژی',\n",
       " 'انرژی پتانسیل  ویژگی\\u200cهای انرژی پتانسیل',\n",
       " 'انستیتو پاستور ایران  بخش\\u200cهای انستیتو',\n",
       " 'انیو موریکونه  زندگی شخصی و درگذشت',\n",
       " 'باشگاه فوتبال آرسنال',\n",
       " 'باشگاه فوتبال استقلال تهران  رسانه و فرهنگ عمومی',\n",
       " 'باشگاه فوتبال اینتر میلان  دوران پس از مورینیو',\n",
       " 'باشگاه فوتبال بارسلونا  رفتن راسل و آمدن بارتومئو (تاکنون-۲۰۱۴)',\n",
       " 'باشگاه فوتبال بارسلونا  رکوردها',\n",
       " 'بمب\\u200cگذاری در دفتر حزب جمهوری اسلامی  اسامی جان\\u200cباختگان',\n",
       " 'بهمن فرمان\\u200cآرا  جایزه\\u200cها و افتخارات',\n",
       " 'بیت\\u200cالعدل اعظم  گوشه\\u200cای از فعالیت\\u200cها',\n",
       " 'بیضی  تعریف بیضی با دو کانون',\n",
       " 'تاریخ افغانستان (پیش از اسلام)  شاهنشاهی کوشانیان',\n",
       " 'تاریخ گرجستان  از دوران مغول تا پایان قرن ۱۸',\n",
       " 'تعادل نش  تاریخچه',\n",
       " 'تیتانیوم دی\\u200cاکسید  فتوکاتالیست',\n",
       " 'تیم برتون  سال\\u200cهای ۱۹۸۰ تا ۱۹۹۰',\n",
       " 'تیم ملی فوتبال ایران  هماوردان',\n",
       " 'تیم ملی فوتبال ایران  ورزشگاه خانگی',\n",
       " 'تیم ملی فوتبال فرانسه  دوران امه ژاکه و زیدان',\n",
       " 'جام ملت\\u200cهای آسیا',\n",
       " 'جغرافیای جشن نوروز',\n",
       " 'جمهوری وایمار  انقلاب نوامبر (۱۹۱۸–۱۹۱۹)',\n",
       " 'جمهوری وایمار  ساختار سیاسی',\n",
       " 'جنبش سیاه\\u200cجامگان  دوره علنی',\n",
       " 'جنگ اسرائیل و لبنان (۲۰۰۶)  آغاز درگیری',\n",
       " 'جنگ کره',\n",
       " 'جو بایدن  معاونت ریاست\\u200cجمهوری (۲۰۰۹–۲۰۱۷)',\n",
       " 'جو بایدن  نقش جو بایدن در انتخابات ۲۰۱۶',\n",
       " 'جیوه  تغییر پذیری جیوه',\n",
       " 'حافظ  به\\u200cکارگیری صور خیال',\n",
       " 'حافظ  دربارهٔ حافظ',\n",
       " 'حافظه رایانه  ثبّات (رجیستر) و حافظهٔ پنهان',\n",
       " 'حاکمیت ایران بر جزایر سه\\u200cگانه  پیشینه مناقشه با استعمار بریتانیا',\n",
       " 'حداقل مربعات خطی  هندسه روش کمترین مربعات معمولی',\n",
       " 'حسن مجتبی  تدارک سپاه در مقابل معاویه',\n",
       " 'حسین واعظ کاشفی  مذهب',\n",
       " 'حقوق بشر  اعلامیه جهانی حقوق بشر',\n",
       " 'خرس  رفتارشناسی و نحوه زندگی',\n",
       " 'خرس قطبی  زیست\\u200cشناسی و رفتار',\n",
       " 'خرس قطبی  ویژگی\\u200cهای فیزیکی',\n",
       " 'دانشگاه صنعتی اصفهان  تاریخچه دانشگاه',\n",
       " 'دانشگاه علم و صنعت ایران  مدیران دانشگاه',\n",
       " 'داوود  شورش ابشالوم',\n",
       " 'دشتی (آواز)  نمونه\\u200cها',\n",
       " 'دیابت  اختلالات پیش\\u200cدیابتی',\n",
       " 'دیابت  دیابت بارداری',\n",
       " 'راه شیری  نمای ظاهری',\n",
       " 'رباعیات خیام  نسخه\\u200cهای اروپایی',\n",
       " 'رضا عطاران  سینما',\n",
       " 'رگرسیون خطی  تجزیه و تحلیل مدل',\n",
       " 'رگرسیون خطی  مدل سلسله مراتبی',\n",
       " 'ریشه دوم  خواص',\n",
       " 'زبان فارسی در هند  دیرینگی پیوند زبانی ایران و هندوستان',\n",
       " 'زرتشت  یونانیان باستان',\n",
       " 'سازمان توسعه صنعتی ملل متحد  تشکیلات و اهداف',\n",
       " 'سعید نفیسی  متون فارسی',\n",
       " 'سکته مغزی',\n",
       " 'شاه اسماعیل دوم  سیاست امنیتی و قضایی',\n",
       " 'شرکت نفت ایران و انگلیس  تاریخچه',\n",
       " 'شهرستان آباده  شاه اسماعیل سوم در آباده',\n",
       " 'شهرستان تنکابن  تاریخچه جداسازی شهرستان تنکابن',\n",
       " 'شورای نگهبان',\n",
       " 'شوقی افندی  دستاوردها',\n",
       " 'شیر و خورشید  دوران صفوی',\n",
       " 'شیر و خورشید  پس از انقلاب ۱۳۵۷',\n",
       " 'شیراز  فرودگاه',\n",
       " 'صدام حسین  پیش\\u200cزمینه کشمکش با کویت',\n",
       " 'امام علی بن موسی الرضا ع لقب رضا',\n",
       " 'علی دایی  بایرن مونیخ',\n",
       " 'علی دایی  مرحله انتخابی',\n",
       " 'علی پروین  سال ۱۳۷۲، برکناری پروین از تیم ملی',\n",
       " 'عید فطر  در مالزی',\n",
       " 'غار علی\\u200cصدر  تاریخچه',\n",
       " 'حضرت فاطمه زهرا س',\n",
       " 'فرودگاه بین\\u200cالمللی امام خمینی  پیشینه',\n",
       " 'فشار خون بالا  بحران\\u200cهای پرفشاری خون',\n",
       " 'فشار خون بالا  تشخیص',\n",
       " 'قریش (سوره)  رابطهٔ سوره\\u200cهای فیل و قریش',\n",
       " 'قضیه رمزی  اعداد رمزی',\n",
       " 'لامپ رشته\\u200cای',\n",
       " 'لطفعلی\\u200cخان زند',\n",
       " 'لطفعلی\\u200cخان زند  بعد از محاصره',\n",
       " 'لطفعلی\\u200cخان زند  توطئه\\u200cچینی در شیراز',\n",
       " 'لطفعلی\\u200cخان زند  پشت دیوارهای شیراز',\n",
       " 'ماری کوری  جایزه\\u200cهای نوبل',\n",
       " 'محاصره انطاکیه (۱۰۹۸)  بهار',\n",
       " 'محاصره انطاکیه (۱۰۹۸)  زمستان',\n",
       " 'محمد مصدق  مخالفان و منتقدان عملکرد ۲۸ ماهه دولت مصدق',\n",
       " 'مردم تالش  تاریخ مردم تالش',\n",
       " 'مریخ',\n",
       " 'معادله شرودینگر  روشی برای معادله',\n",
       " 'معادله لاپلاس  تابع گرین',\n",
       " 'مغول  تموچین یا چنگیز',\n",
       " 'مقاومت و رسانایی الکتریکی  ابر رسانا',\n",
       " 'مهندسی نرم\\u200cافزار',\n",
       " 'موسی  سرگردانی در بیابان',\n",
       " 'موسیقی راک',\n",
       " 'میخائیل گورباچف  کودتا و فروپاشی شوروی',\n",
       " 'نادرشاه  جنگ با عثمانی',\n",
       " 'نادرشاه  درگیری با تهماسب',\n",
       " 'ناسا  ایستگاه فضایی بین\\u200cالمللی (۱۹۹۸–اکنون)',\n",
       " 'نرون  کودتایی مخفیانه در برابر مادر و قتل آگریپینا',\n",
       " 'نروژ',\n",
       " 'نسبیت عام  تعریف و ویژگی\\u200cهای پایه\\u200cای',\n",
       " 'نستعلیق',\n",
       " 'نستعلیق  پیدایش و گسترش',\n",
       " 'نظریه اصل موضوعی مجموعه\\u200cها  سازگاری و عدم وابستگی در ZFC',\n",
       " 'نظریه نسبیت  نسبیت عام',\n",
       " 'نلسون ماندلا  آغاز',\n",
       " 'نهنگ قاتل  هوش',\n",
       " 'نوا (دستگاه موسیقی)  گوشه\\u200cها',\n",
       " 'نیروهای محور  اتحاد دانوب، اختلاف بر سر اتریش',\n",
       " 'هاینریش هیملر  استحکام قدرت',\n",
       " 'هاینریش هیملر  رابطه با هیتلر',\n",
       " 'هسته لینوکس',\n",
       " 'هسته لینوکس  درگیری\\u200cهای جامعه توسعه',\n",
       " 'هسته لینوکس  مدل توسعه',\n",
       " 'هم\\u200cارزی جرم و انرژی  کاربست\\u200cپذیری فرمول',\n",
       " 'هندسه جبری',\n",
       " 'هوش مصنوعی  تاریخچه',\n",
       " 'واپاشی هسته\\u200cای  پایداری و ناپایداری ایزوتوپ\\u200cها',\n",
       " 'وشمگیر  وضعیت سیاسی-اجتماعی قرن چهارم هجری',\n",
       " 'ولایت قندهار  تمدن مندیگک',\n",
       " 'ولفگانگ آمادئوس موتسارت  موتسارت در وین',\n",
       " 'ونکوور  سیستم حمل و نقل شهری',\n",
       " 'ونکوور  معماری',\n",
       " 'پروین اعتصامی',\n",
       " 'پرچم ایران  پیش از پادشاهی پهلوی\\u200cها',\n",
       " 'پیمان کیوتو  اتحادیه اروپا',\n",
       " 'چرخه آب  توصیف',\n",
       " 'چنگیز خان  کودکی',\n",
       " 'چهاردهمین دالایی لاما  اوان زندگی و سابقه',\n",
       " 'کارل مارکس  اقتصاد، تاریخ و جامعه',\n",
       " 'گرجستان  تاریخ',\n",
       " 'یانی  تأثیرپذیری\\u200cهای موسیقایی',\n",
       " 'یونسکو  فعالیت\\u200cها']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dataset.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "files = glob('../dataset/IR_dataset/*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../dataset/IR_dataset/2048.txt',\n",
       " '../dataset/IR_dataset/2404.txt',\n",
       " '../dataset/IR_dataset/661.txt',\n",
       " '../dataset/IR_dataset/1252.txt',\n",
       " '../dataset/IR_dataset/726.txt',\n",
       " '../dataset/IR_dataset/3029.txt',\n",
       " '../dataset/IR_dataset/329.txt',\n",
       " '../dataset/IR_dataset/1481.txt',\n",
       " '../dataset/IR_dataset/1511.txt',\n",
       " '../dataset/IR_dataset/1127.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_chars = [\n",
    "  'آ',\n",
    "  'أ',\n",
    "  'ؤ',\n",
    "  'إ',\n",
    "  'ئ',\n",
    "  'ا',\n",
    "  'ب',\n",
    "  'ة',\n",
    "  'ت',\n",
    "  'ث',\n",
    "  'ج',\n",
    "  'ح',\n",
    "  'خ',\n",
    "  'د',\n",
    "  'ذ',\n",
    "  'ر',\n",
    "  'ز',\n",
    "  'س',\n",
    "  'ش',\n",
    "  'ص',\n",
    "  'ض',\n",
    "  'ط',\n",
    "  'ظ',\n",
    "  'ع',\n",
    "  'غ',\n",
    "  'ف',\n",
    "  'ق',\n",
    "  'ك',\n",
    "  'ل',\n",
    "  'م',\n",
    "  'ن',\n",
    "  'ه',\n",
    "  'و',\n",
    "  'ى',\n",
    "  'ي',\n",
    "  '٠',\n",
    "  '١',\n",
    "  '٢',\n",
    "  '٣',\n",
    "  '٤',\n",
    "  '٥',\n",
    "  '٦',\n",
    "  '٧',\n",
    "  '٨',\n",
    "  '٩',\n",
    "  'چ',\n",
    "  'ژ',\n",
    "  'ک',\n",
    "  'گ',\n",
    "  'ھ',\n",
    "  'ی',\n",
    "  '۰',\n",
    "  '۱',\n",
    "  '۲',\n",
    "  '۳',\n",
    "  '۴',\n",
    "  '۵',\n",
    "  '۶',\n",
    "  '۷',\n",
    "  '۸',\n",
    "  '۹',\n",
    "  '\\u200c',\n",
    "  '\\u200d',\n",
    "  '\\u200e',\n",
    "  '\\u200f',\n",
    "  'پ',\n",
    "  'ﭼ',\n",
    "  'ﯽ',\n",
    "  'ﯾ',\n",
    "  'ﯿ',\n",
    "  'ﷲ',\n",
    "  'ﺄ',\n",
    "  'ﺆ',\n",
    "  'ﺋ',\n",
    "  'ﺎ',\n",
    "  'ﺑ',\n",
    "  'ﺔ',\n",
    "  'ﺗ',\n",
    "  'ﺘ',\n",
    "  'ﺧ',\n",
    "  'ﺪ',\n",
    "  'ﺮ',\n",
    "  'ﺳ',\n",
    "  'ﺴ',\n",
    "  'ﺿ',\n",
    "  'ﻋ',\n",
    "  'ﻌ',\n",
    "  'ﻗ',\n",
    "  'ﻠ',\n",
    "  'ﻣ',\n",
    "  'ﻨ',\n",
    "  'ﻼ',\n",
    "  '￼']\n",
    "\n",
    "trans_chars = [\n",
    "    'ً',\n",
    "  'ٌ',\n",
    "  'ٍ',\n",
    "  'َ',\n",
    "  'ُ',\n",
    "  'ِ',\n",
    "  'ّ',\n",
    "  'ْ',\n",
    "  'ٓ',\n",
    "  'ٔ',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set()\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "for file in files:\n",
    "    with open(file) as fp:\n",
    "        lines = fp.readlines()\n",
    "        for line in lines:\n",
    "            line = re.sub('[' + ''.join(trans_chars) + ']', '', line)\n",
    "            line = re.sub('[^' + ''.join(allowed_chars) + ']', ' ', line)\n",
    "#             line = re.sub('ئ', 'ی', line)\n",
    "            words.update(nltk.tokenize.word_tokenize(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75011,\n",
       " ['آ',\n",
       "  'آآآن',\n",
       "  'آئتیوس',\n",
       "  'آئدیلس',\n",
       "  'آئرا',\n",
       "  'آئلیوس',\n",
       "  'آئه',\n",
       "  'آئودا',\n",
       "  'آئورا',\n",
       "  'آئوراهایی',\n",
       "  'آئورت',\n",
       "  'آئورنوس',\n",
       "  'آئوسان',\n",
       "  'آئوسان\\u200cها',\n",
       "  'آئولوس',\n",
       "  'آئین',\n",
       "  'آئینه',\n",
       "  'آئینه\\u200cکاری',\n",
       "  'آئینی',\n",
       "  'آئین\\u200cشان'])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words), sorted(words)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = set(' '.join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(634,\n",
       " [' ',\n",
       "  '!',\n",
       "  '#',\n",
       "  '$',\n",
       "  '%',\n",
       "  '&',\n",
       "  \"'\",\n",
       "  '(',\n",
       "  ')',\n",
       "  '*',\n",
       "  '+',\n",
       "  ',',\n",
       "  '-',\n",
       "  '.',\n",
       "  '/',\n",
       "  '0',\n",
       "  '1',\n",
       "  '2',\n",
       "  '3',\n",
       "  '4',\n",
       "  '5',\n",
       "  '6',\n",
       "  '7',\n",
       "  '8',\n",
       "  '9',\n",
       "  ':',\n",
       "  ';',\n",
       "  '<',\n",
       "  '=',\n",
       "  '>',\n",
       "  '?',\n",
       "  'A',\n",
       "  'B',\n",
       "  'C',\n",
       "  'D',\n",
       "  'E',\n",
       "  'F',\n",
       "  'G',\n",
       "  'H',\n",
       "  'I',\n",
       "  'J',\n",
       "  'K',\n",
       "  'L',\n",
       "  'M',\n",
       "  'N',\n",
       "  'O',\n",
       "  'P',\n",
       "  'Q',\n",
       "  'R',\n",
       "  'S',\n",
       "  'T',\n",
       "  'U',\n",
       "  'V',\n",
       "  'W',\n",
       "  'X',\n",
       "  'Y',\n",
       "  'Z',\n",
       "  '[',\n",
       "  '\\\\',\n",
       "  ']',\n",
       "  '^',\n",
       "  '_',\n",
       "  '`',\n",
       "  'a',\n",
       "  'b',\n",
       "  'c',\n",
       "  'd',\n",
       "  'e',\n",
       "  'f',\n",
       "  'g',\n",
       "  'h',\n",
       "  'i',\n",
       "  'j',\n",
       "  'k',\n",
       "  'l',\n",
       "  'm',\n",
       "  'n',\n",
       "  'o',\n",
       "  'p',\n",
       "  'q',\n",
       "  'r',\n",
       "  's',\n",
       "  't',\n",
       "  'u',\n",
       "  'v',\n",
       "  'w',\n",
       "  'x',\n",
       "  'y',\n",
       "  'z',\n",
       "  '{',\n",
       "  '|',\n",
       "  '}',\n",
       "  '~',\n",
       "  '§',\n",
       "  '«',\n",
       "  '¬',\n",
       "  '\\xad',\n",
       "  '°',\n",
       "  '±',\n",
       "  '²',\n",
       "  '³',\n",
       "  'µ',\n",
       "  '·',\n",
       "  '»',\n",
       "  '½',\n",
       "  'À',\n",
       "  'Á',\n",
       "  'Å',\n",
       "  'Æ',\n",
       "  'È',\n",
       "  'É',\n",
       "  'Í',\n",
       "  'Ò',\n",
       "  'Ó',\n",
       "  'Ö',\n",
       "  '×',\n",
       "  'Ø',\n",
       "  'Ú',\n",
       "  'Ü',\n",
       "  'ß',\n",
       "  'à',\n",
       "  'á',\n",
       "  'â',\n",
       "  'ä',\n",
       "  'å',\n",
       "  'æ',\n",
       "  'ç',\n",
       "  'è',\n",
       "  'é',\n",
       "  'ê',\n",
       "  'ë',\n",
       "  'í',\n",
       "  'î',\n",
       "  'ð',\n",
       "  'ñ',\n",
       "  'ò',\n",
       "  'ó',\n",
       "  'ô',\n",
       "  'ö',\n",
       "  'ø',\n",
       "  'ú',\n",
       "  'û',\n",
       "  'ü',\n",
       "  'ý',\n",
       "  'ā',\n",
       "  'ă',\n",
       "  'ć',\n",
       "  'Č',\n",
       "  'č',\n",
       "  'Ď',\n",
       "  'ď',\n",
       "  'đ',\n",
       "  'Ē',\n",
       "  'ē',\n",
       "  'Ĕ',\n",
       "  'ę',\n",
       "  'ğ',\n",
       "  'ħ',\n",
       "  'ī',\n",
       "  'ĭ',\n",
       "  'İ',\n",
       "  'ı',\n",
       "  'ľ',\n",
       "  'ł',\n",
       "  'ń',\n",
       "  'ň',\n",
       "  'ŋ',\n",
       "  'ō',\n",
       "  'ő',\n",
       "  'œ',\n",
       "  'Ś',\n",
       "  'ś',\n",
       "  'ş',\n",
       "  'Š',\n",
       "  'š',\n",
       "  'ţ',\n",
       "  'Ť',\n",
       "  'ť',\n",
       "  'ũ',\n",
       "  'ū',\n",
       "  'ż',\n",
       "  'Ž',\n",
       "  'ž',\n",
       "  'Ə',\n",
       "  'ƒ',\n",
       "  'ơ',\n",
       "  'ǎ',\n",
       "  'ǐ',\n",
       "  'ǰ',\n",
       "  'ɑ',\n",
       "  'ə',\n",
       "  'ɛ',\n",
       "  'ɡ',\n",
       "  'ɪ',\n",
       "  'ʃ',\n",
       "  'ʼ',\n",
       "  'ʾ',\n",
       "  'ˀ',\n",
       "  'ˈ',\n",
       "  'ː',\n",
       "  '˚',\n",
       "  '́',\n",
       "  'Α',\n",
       "  'Β',\n",
       "  'Γ',\n",
       "  'Δ',\n",
       "  'Ζ',\n",
       "  'Θ',\n",
       "  'Κ',\n",
       "  'Λ',\n",
       "  'Μ',\n",
       "  'Ν',\n",
       "  'Ο',\n",
       "  'Φ',\n",
       "  'Ψ',\n",
       "  'Ω',\n",
       "  'ά',\n",
       "  'έ',\n",
       "  'ή',\n",
       "  'ί',\n",
       "  'α',\n",
       "  'β',\n",
       "  'γ',\n",
       "  'δ',\n",
       "  'ε',\n",
       "  'η',\n",
       "  'θ',\n",
       "  'ι',\n",
       "  'κ',\n",
       "  'λ',\n",
       "  'μ',\n",
       "  'ν',\n",
       "  'ξ',\n",
       "  'ο',\n",
       "  'π',\n",
       "  'ρ',\n",
       "  'ς',\n",
       "  'σ',\n",
       "  'τ',\n",
       "  'υ',\n",
       "  'φ',\n",
       "  'χ',\n",
       "  'ψ',\n",
       "  'ω',\n",
       "  'ό',\n",
       "  'ύ',\n",
       "  'ϐ',\n",
       "  'Ϸ',\n",
       "  'ϸ',\n",
       "  'Є',\n",
       "  'В',\n",
       "  'Г',\n",
       "  'Д',\n",
       "  'З',\n",
       "  'М',\n",
       "  'П',\n",
       "  'Р',\n",
       "  'С',\n",
       "  'Я',\n",
       "  'а',\n",
       "  'б',\n",
       "  'в',\n",
       "  'г',\n",
       "  'д',\n",
       "  'е',\n",
       "  'и',\n",
       "  'й',\n",
       "  'к',\n",
       "  'л',\n",
       "  'м',\n",
       "  'н',\n",
       "  'о',\n",
       "  'п',\n",
       "  'р',\n",
       "  'с',\n",
       "  'т',\n",
       "  'у',\n",
       "  'ч',\n",
       "  'ш',\n",
       "  'ы',\n",
       "  'ю',\n",
       "  'я',\n",
       "  'ё',\n",
       "  'Բ',\n",
       "  'Ձ',\n",
       "  'ա',\n",
       "  'ե',\n",
       "  'զ',\n",
       "  'ր',\n",
       "  'ւ',\n",
       "  'ְ',\n",
       "  'ִ',\n",
       "  'ַ',\n",
       "  'ּ',\n",
       "  'ב',\n",
       "  'ד',\n",
       "  'ה',\n",
       "  'ו',\n",
       "  'י',\n",
       "  'ל',\n",
       "  'ם',\n",
       "  'ן',\n",
       "  'ס',\n",
       "  'ע',\n",
       "  'ף',\n",
       "  'ק',\n",
       "  'ר',\n",
       "  'ש',\n",
       "  'ת',\n",
       "  '׳',\n",
       "  '،',\n",
       "  '؛',\n",
       "  '؟',\n",
       "  'ء',\n",
       "  'آ',\n",
       "  'أ',\n",
       "  'ؤ',\n",
       "  'إ',\n",
       "  'ئ',\n",
       "  'ا',\n",
       "  'ب',\n",
       "  'ة',\n",
       "  'ت',\n",
       "  'ث',\n",
       "  'ج',\n",
       "  'ح',\n",
       "  'خ',\n",
       "  'د',\n",
       "  'ذ',\n",
       "  'ر',\n",
       "  'ز',\n",
       "  'س',\n",
       "  'ش',\n",
       "  'ص',\n",
       "  'ض',\n",
       "  'ط',\n",
       "  'ظ',\n",
       "  'ع',\n",
       "  'غ',\n",
       "  'ـ',\n",
       "  'ف',\n",
       "  'ق',\n",
       "  'ك',\n",
       "  'ل',\n",
       "  'م',\n",
       "  'ن',\n",
       "  'ه',\n",
       "  'و',\n",
       "  'ى',\n",
       "  'ي',\n",
       "  'ً',\n",
       "  'ٌ',\n",
       "  'ٍ',\n",
       "  'َ',\n",
       "  'ُ',\n",
       "  'ِ',\n",
       "  'ّ',\n",
       "  'ْ',\n",
       "  'ٓ',\n",
       "  'ٔ',\n",
       "  '٠',\n",
       "  '١',\n",
       "  '٢',\n",
       "  '٣',\n",
       "  '٤',\n",
       "  '٥',\n",
       "  '٦',\n",
       "  '٧',\n",
       "  '٨',\n",
       "  '٩',\n",
       "  '٪',\n",
       "  '٫',\n",
       "  '٬',\n",
       "  'ٰ',\n",
       "  'ٱ',\n",
       "  'ٴ',\n",
       "  'ٹ',\n",
       "  'پ',\n",
       "  'څ',\n",
       "  'چ',\n",
       "  'ڈ',\n",
       "  'ڑ',\n",
       "  'ڕ',\n",
       "  'ژ',\n",
       "  'ک',\n",
       "  'گ',\n",
       "  'ڵ',\n",
       "  'ں',\n",
       "  'ھ',\n",
       "  'ۀ',\n",
       "  'ۆ',\n",
       "  'ی',\n",
       "  'ێ',\n",
       "  'ۏ',\n",
       "  'ے',\n",
       "  'ۖ',\n",
       "  '۰',\n",
       "  '۱',\n",
       "  '۲',\n",
       "  '۳',\n",
       "  '۴',\n",
       "  '۵',\n",
       "  '۶',\n",
       "  '۷',\n",
       "  '۸',\n",
       "  '۹',\n",
       "  'ݔ',\n",
       "  'ࢣ',\n",
       "  'ं',\n",
       "  'ई',\n",
       "  'क',\n",
       "  'ज',\n",
       "  'ण',\n",
       "  'त',\n",
       "  'न',\n",
       "  'प',\n",
       "  'ब',\n",
       "  'म',\n",
       "  'र',\n",
       "  'व',\n",
       "  'श',\n",
       "  'ष',\n",
       "  'स',\n",
       "  'ा',\n",
       "  'ु',\n",
       "  'े',\n",
       "  '्',\n",
       "  'க',\n",
       "  'ங',\n",
       "  'ச',\n",
       "  'ட',\n",
       "  'ண',\n",
       "  'ப',\n",
       "  'ர',\n",
       "  'ா',\n",
       "  'ி',\n",
       "  'ு',\n",
       "  'ூ',\n",
       "  '்',\n",
       "  'ด',\n",
       "  'ว',\n",
       "  'ส',\n",
       "  'ั',\n",
       "  'ี',\n",
       "  'ད',\n",
       "  'བ',\n",
       "  'ོ',\n",
       "  'ა',\n",
       "  'ბ',\n",
       "  'გ',\n",
       "  'თ',\n",
       "  'მ',\n",
       "  'ო',\n",
       "  'რ',\n",
       "  'ს',\n",
       "  'უ',\n",
       "  'ჯ',\n",
       "  'ᴓ',\n",
       "  'ᴨ',\n",
       "  'ḏ',\n",
       "  'ḥ',\n",
       "  'ṣ',\n",
       "  'ả',\n",
       "  'ầ',\n",
       "  'ệ',\n",
       "  'ộ',\n",
       "  'ἀ',\n",
       "  'ἐ',\n",
       "  'ἡ',\n",
       "  'Ῥ',\n",
       "  'ῶ',\n",
       "  '\\u200c',\n",
       "  '\\u200d',\n",
       "  '\\u200e',\n",
       "  '\\u200f',\n",
       "  '‐',\n",
       "  '–',\n",
       "  '—',\n",
       "  '‘',\n",
       "  '’',\n",
       "  '“',\n",
       "  '”',\n",
       "  '†',\n",
       "  '•',\n",
       "  '…',\n",
       "  '\\u202b',\n",
       "  '\\u202c',\n",
       "  '′',\n",
       "  '″',\n",
       "  '\\u206f',\n",
       "  '€',\n",
       "  '←',\n",
       "  '→',\n",
       "  '↦',\n",
       "  '↵',\n",
       "  '∆',\n",
       "  '∇',\n",
       "  '∈',\n",
       "  '∑',\n",
       "  '−',\n",
       "  '√',\n",
       "  '∞',\n",
       "  '∧',\n",
       "  '∨',\n",
       "  '∩',\n",
       "  '∪',\n",
       "  '≈',\n",
       "  '≠',\n",
       "  '≡',\n",
       "  '≤',\n",
       "  '≥',\n",
       "  '⊆',\n",
       "  '□',\n",
       "  '◇',\n",
       "  '♠',\n",
       "  '♣',\n",
       "  '♥',\n",
       "  '♦',\n",
       "  '《',\n",
       "  '》',\n",
       "  'い',\n",
       "  'う',\n",
       "  'が',\n",
       "  'こ',\n",
       "  'し',\n",
       "  'す',\n",
       "  'せ',\n",
       "  'ち',\n",
       "  'な',\n",
       "  'に',\n",
       "  'は',\n",
       "  'り',\n",
       "  'る',\n",
       "  'わ',\n",
       "  'ん',\n",
       "  'ァ',\n",
       "  'コ',\n",
       "  'フ',\n",
       "  'ミ',\n",
       "  'ン',\n",
       "  '中',\n",
       "  '人',\n",
       "  '你',\n",
       "  '元',\n",
       "  '共',\n",
       "  '华',\n",
       "  '南',\n",
       "  '合',\n",
       "  '同',\n",
       "  '和',\n",
       "  '国',\n",
       "  '國',\n",
       "  '大',\n",
       "  '女',\n",
       "  '好',\n",
       "  '子',\n",
       "  '帝',\n",
       "  '待',\n",
       "  '意',\n",
       "  '成',\n",
       "  '持',\n",
       "  '民',\n",
       "  '気',\n",
       "  '熊',\n",
       "  '猫',\n",
       "  '知',\n",
       "  '磁',\n",
       "  '結',\n",
       "  '統',\n",
       "  '腐',\n",
       "  '話',\n",
       "  '貴',\n",
       "  '質',\n",
       "  '越',\n",
       "  '連',\n",
       "  '道',\n",
       "  '電',\n",
       "  '霜',\n",
       "  '녕',\n",
       "  '세',\n",
       "  '안',\n",
       "  '요',\n",
       "  '하',\n",
       "  'ﭼ',\n",
       "  'ﯽ',\n",
       "  'ﯾ',\n",
       "  'ﯿ',\n",
       "  'ﷲ',\n",
       "  'ﺀ',\n",
       "  'ﺄ',\n",
       "  'ﺆ',\n",
       "  'ﺋ',\n",
       "  'ﺎ',\n",
       "  'ﺑ',\n",
       "  'ﺔ',\n",
       "  'ﺗ',\n",
       "  'ﺘ',\n",
       "  'ﺧ',\n",
       "  'ﺪ',\n",
       "  'ﺮ',\n",
       "  'ﺳ',\n",
       "  'ﺴ',\n",
       "  'ﺿ',\n",
       "  'ﻋ',\n",
       "  'ﻌ',\n",
       "  'ﻗ',\n",
       "  'ﻠ',\n",
       "  'ﻣ',\n",
       "  'ﻨ',\n",
       "  'ﻼ',\n",
       "  '￼',\n",
       "  '𐎡',\n",
       "  '𐎺',\n",
       "  '𐎼',\n",
       "  '𐏃',\n",
       "  '𐬀',\n",
       "  '𐬎',\n",
       "  '𐬙',\n",
       "  '𐬚',\n",
       "  '𐬭',\n",
       "  '𐬰',\n",
       "  '𐬱',\n",
       "  '𐭅',\n",
       "  '𐭇',\n",
       "  '𐭊',\n",
       "  '𐭍',\n",
       "  '𐭓',\n",
       "  '𐭔',\n",
       "  '𐭕',\n",
       "  '𐭥',\n",
       "  '𐭦',\n",
       "  '𐭫',\n",
       "  '𐭱',\n",
       "  '𐭲'])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chars), sorted(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format('../dataset/word2vec.model-skipgram-size=200-window=5.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_word2vec_format('/tmp/word2vec.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = []\n",
    "vectors  = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/tmp/word2vec.txt\") as fp:\n",
    "    for i, line in enumerate(fp):\n",
    "        if i == 0:\n",
    "            print(line)\n",
    "            continue\n",
    "        spls = line.split()\n",
    "        word = spls[0]\n",
    "        vector = [float(s) for s in spls[1:201]]\n",
    "        vocab.append(word)\n",
    "        vectors.append(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = np.array(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"w2v/word2vec.npy\", array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240547, 200)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"w2v/vocab.txt\", 'w') as fw:\n",
    "    for w in vocab:\n",
    "        fw.write(w + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "word2vec = np.load(\"w2v/word2vec.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('w2v/vocab.txt') as fp:\n",
    "    vocab = [l.strip() for l in fp.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['در',\n",
       " 'و',\n",
       " '<NUM>',\n",
       " 'به',\n",
       " 'از',\n",
       " 'که',\n",
       " 'است',\n",
       " 'این',\n",
       " 'را',\n",
       " 'با',\n",
       " 'یک',\n",
       " 'سال',\n",
       " 'آن',\n",
       " 'برای',\n",
       " 'شد',\n",
       " 'بود',\n",
       " 'ایران',\n",
       " 'دارد',\n",
       " 'او',\n",
       " 'بر']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240547"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17582,\n",
       " ['۸۶به',\n",
       "  'واسای',\n",
       "  'ماه۱۲۸۶',\n",
       "  '۱۲۵۷',\n",
       "  'سنتوریون\\u200cها',\n",
       "  'هفتورنگ',\n",
       "  'درک\\u200cشان',\n",
       "  'طرحواره\\u200cهایی',\n",
       "  'کریوان',\n",
       "  'هلفتا',\n",
       "  'دایرهها',\n",
       "  'بدعت\\u200cگزاران',\n",
       "  'کاکوان',\n",
       "  '۶۵۵',\n",
       "  'مهارناشدنی',\n",
       "  'قرون۱۹',\n",
       "  'فن\\u200cشناسی',\n",
       "  'ماهاویشنو',\n",
       "  'نیمه\\u200cارباب\\u200cرعیتی',\n",
       "  'الغلیل'])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import sample\n",
    "diff = words.difference(set(vocab))\n",
    "len(diff), sample(diff, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vocab = list(words)\n",
    "vocab_r = {k:i for i,k in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_word2vec = np.empty((len(new_vocab), 200), dtype=np.float64)\n",
    "new_word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, w in enumerate(new_vocab):\n",
    "    if w in vocab_r:\n",
    "        new_word2vec[i] = word2vec[vocab_r[w]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'هنگفنی'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_vocab[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.3484211 ,  0.23893002,  0.01450732, ..., -0.34313798,\n",
       "        -0.10281939, -0.10998372],\n",
       "       [-0.446034  ,  0.36105067, -0.24115418, ...,  0.5980322 ,\n",
       "         0.15938091, -0.24276495],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ...,\n",
       "       [-0.20092697,  0.22308755,  0.53070813, ..., -0.01048616,\n",
       "         0.10839605,  0.050221  ],\n",
       "       [ 0.10551469,  0.31734204, -0.21101032, ..., -0.25399914,\n",
       "         0.21805623, -0.15031986],\n",
       "       [-0.11965861,  0.09617905, -0.16568883, ..., -0.1228699 ,\n",
       "        -0.02177574,  0.15190113]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('w2v/vocab2.txt', 'w') as fw:\n",
    "    for w in new_vocab:\n",
    "        fw.write(w + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"w2v/word2vec2.npy\", new_word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open('pos/words.txt') as fp:\n",
    "    words_all = [line.strip() for line in fp.readlines()]\n",
    "word2int = {k:i for i, k in enumerate(words_all)}\n",
    "VOCAB_SIZE = len(word2int) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3535"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2int['آب\\u200cها']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open('pos/tags.txt') as fp:\n",
    "    tags_all = [line.strip() for line in fp.readlines()]\n",
    "tag2int = {k:i for i, k in enumerate(tags_all)}\n",
    "TAGS_NO = len(tag2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "pos_tagger = load_model('models/pos_lstm_3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 50)]              0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 50, 100)           7840700   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 50, 160)          115840    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 50, 34)           5474      \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,962,014\n",
      "Trainable params: 7,962,014\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "pos_tagger.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-Idf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "\n",
    "def encode_text(text):\n",
    "    text = re.sub('[' + ''.join(trans_chars) + ']', '', text)\n",
    "    text = re.sub('[^' + ''.join(allowed_chars) + ']', ' ', text)\n",
    "    text = re.sub('ئ', 'ی', text)\n",
    "    text = re.sub('ء', '', text)\n",
    "    text = re.sub('[\\s]+', ' ', text)\n",
    "    tokens = nltk.tokenize.word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "\n",
    "for doc in files:\n",
    "    with open(doc) as fp:\n",
    "        text = fp.read()\n",
    "        text = re.sub('\\n', '', text)\n",
    "        tokens = encode_text(text)\n",
    "        corpus.append(' '.join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3258, 60732)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_words_r = {k:i for i, k in enumerate(vectorizer.get_feature_names())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30680919856295225"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0, tfidf_words_r['پودر']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import save_npz\n",
    "\n",
    "save_npz(\"tfidf/tfidf.npz\", X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tfidf/words.txt', 'w') as fw:\n",
    "    for w in vectorizer.get_feature_names():\n",
    "        fw.write(w + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tfidf/docs.txt', 'w') as fw:\n",
    "    for doc in files:\n",
    "        fw.write(doc + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
