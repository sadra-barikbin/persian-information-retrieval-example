{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "word2vec = np.load(\"w2v/word2vec3.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('w2v/vocab3.txt') as fp:\n",
    "    vocab = [l.strip() for l in fp.readlines()]\n",
    "vocab_r = {k:i for i, k in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58296, (58296, 200))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab), word2vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "docs = glob('../dataset/IR_dataset/*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../dataset/IR_dataset/2048.txt',\n",
       " '../dataset/IR_dataset/2404.txt',\n",
       " '../dataset/IR_dataset/661.txt',\n",
       " '../dataset/IR_dataset/1252.txt',\n",
       " '../dataset/IR_dataset/726.txt',\n",
       " '../dataset/IR_dataset/3029.txt',\n",
       " '../dataset/IR_dataset/329.txt',\n",
       " '../dataset/IR_dataset/1481.txt',\n",
       " '../dataset/IR_dataset/1511.txt',\n",
       " '../dataset/IR_dataset/1127.txt']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_chars = [\n",
    "  'آ',\n",
    "  'أ',\n",
    "  'ؤ',\n",
    "  'إ',\n",
    "  'ئ',\n",
    "  'ا',\n",
    "  'ب',\n",
    "  'ة',\n",
    "  'ت',\n",
    "  'ث',\n",
    "  'ج',\n",
    "  'ح',\n",
    "  'خ',\n",
    "  'د',\n",
    "  'ذ',\n",
    "  'ر',\n",
    "  'ز',\n",
    "  'س',\n",
    "  'ش',\n",
    "  'ص',\n",
    "  'ض',\n",
    "  'ط',\n",
    "  'ظ',\n",
    "  'ع',\n",
    "  'غ',\n",
    "  'ف',\n",
    "  'ق',\n",
    "  'ك',\n",
    "  'ل',\n",
    "  'م',\n",
    "  'ن',\n",
    "  'ه',\n",
    "  'و',\n",
    "  'ى',\n",
    "  'ي',\n",
    "  '٠',\n",
    "  '١',\n",
    "  '٢',\n",
    "  '٣',\n",
    "  '٤',\n",
    "  '٥',\n",
    "  '٦',\n",
    "  '٧',\n",
    "  '٨',\n",
    "  '٩',\n",
    "  'چ',\n",
    "  'ژ',\n",
    "  'ک',\n",
    "  'گ',\n",
    "  'ھ',\n",
    "  'ی',\n",
    "  '۰',\n",
    "  '۱',\n",
    "  '۲',\n",
    "  '۳',\n",
    "  '۴',\n",
    "  '۵',\n",
    "  '۶',\n",
    "  '۷',\n",
    "  '۸',\n",
    "  '۹',\n",
    "#   '\\u200c',\n",
    "  '\\u200d',\n",
    "  '\\u200e',\n",
    "  '\\u200f',\n",
    "  'پ',\n",
    "  'ﭼ',\n",
    "  'ﯽ',\n",
    "  'ﯾ',\n",
    "  'ﯿ',\n",
    "  'ﷲ',\n",
    "  'ﺄ',\n",
    "  'ﺆ',\n",
    "  'ﺋ',\n",
    "  'ﺎ',\n",
    "  'ﺑ',\n",
    "  'ﺔ',\n",
    "  'ﺗ',\n",
    "  'ﺘ',\n",
    "  'ﺧ',\n",
    "  'ﺪ',\n",
    "  'ﺮ',\n",
    "  'ﺳ',\n",
    "  'ﺴ',\n",
    "  'ﺿ',\n",
    "  'ﻋ',\n",
    "  'ﻌ',\n",
    "  'ﻗ',\n",
    "  'ﻠ',\n",
    "  'ﻣ',\n",
    "  'ﻨ',\n",
    "  'ﻼ',\n",
    "  '￼']\n",
    "\n",
    "trans_chars = [\n",
    "  'ً',\n",
    "  'ٌ',\n",
    "  'ٍ',\n",
    "  'َ',\n",
    "  'ُ',\n",
    "  'ِ',\n",
    "  'ّ',\n",
    "  'ْ',\n",
    "  'ٓ',\n",
    "  'ٔ',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hazm import Normalizer\n",
    "\n",
    "normalizer = Normalizer()\n",
    "\n",
    "word_seqs = []\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import re\n",
    "\n",
    "for doc in docs:\n",
    "    with open(doc) as fp:\n",
    "        lines = fp.readlines()\n",
    "        for line in lines:\n",
    "            line = normalizer.normalize(line)\n",
    "            line = ' '.join([a.strip() for a in re.split(\"([۰-۹]+)\", line) if a])\n",
    "            line = re.sub('[' + ''.join(trans_chars) + ']', '', line)\n",
    "            line = re.sub('[^' + ''.join(allowed_chars) + ']', ' ', line)\n",
    "            word_seqs += [[vocab_r[w] for w in word_tokenize(s)] for s in sent_tokenize(line)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 64\n",
    "SEED = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "sequences = []\n",
    "\n",
    "for seq in word_seqs:\n",
    "    sequences += list(chunks(seq, SEQ_LEN))\n",
    "    \n",
    "sequences = pad_sequences(sequences, maxlen=SEQ_LEN, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53149, 64)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
    "\n",
    "window_size = 2\n",
    "positive_skip_grams, _ = skipgrams(word_seqs[0], \n",
    "                                   vocabulary_size=len(vocab),\n",
    "                                   window_size=window_size,\n",
    "                                   negative_samples=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25695, 46405): (کوره, را)\n",
      "(26270, 53439): (و, شود)\n",
      "(21776, 34579): (چگالی, در)\n",
      "(7294, 33599): (باید, شده)\n",
      "(5527, 27069): (مهم, از)\n"
     ]
    }
   ],
   "source": [
    "for target, context in positive_skip_grams[:5]:\n",
    "    print(f\"({target}, {context}): ({vocab[target]}, {vocab[context]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
    "    # Elements of each training example are appended to these lists.\n",
    "    targets, contexts, labels = [], [], []\n",
    "\n",
    "    # Build the sampling table for vocab_size tokens.\n",
    "    sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "\n",
    "    # Iterate over all sequences (sentences) in dataset.\n",
    "    for sequence in tqdm.tqdm(sequences):\n",
    "\n",
    "        # Generate positive skip-gram pairs for a sequence (sentence).\n",
    "        positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "              sequence,\n",
    "              vocabulary_size=vocab_size,\n",
    "              sampling_table=sampling_table,\n",
    "              window_size=window_size,\n",
    "              negative_samples=0)\n",
    "\n",
    "        # Iterate over each positive skip-gram pair to produce training examples\n",
    "        # with positive context word and negative samples.\n",
    "        for target_word, context_word in positive_skip_grams:\n",
    "            context_class = tf.expand_dims(\n",
    "              tf.constant([context_word], dtype=\"int64\"), 1)\n",
    "            negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "              true_classes=context_class,\n",
    "              num_true=1,\n",
    "              num_sampled=num_ns,\n",
    "              unique=True,\n",
    "              range_max=vocab_size,\n",
    "              seed=seed,\n",
    "              name=\"negative_sampling\")\n",
    "\n",
    "            # Build context and label vectors (for one target word)\n",
    "            negative_sampling_candidates = tf.expand_dims(\n",
    "              negative_sampling_candidates, 1)\n",
    "\n",
    "            context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
    "            label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "\n",
    "            # Append each element from the training example to global lists.\n",
    "            targets.append(target_word)\n",
    "            contexts.append(context)\n",
    "            labels.append(label)\n",
    "\n",
    "    return targets, contexts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 19522/53149 [40:01<6:59:11,  1.34it/s]    "
     ]
    }
   ],
   "source": [
    "targets, contexts, labels = generate_training_data(\n",
    "    sequences=sequences,\n",
    "    window_size=2,\n",
    "    num_ns=4,\n",
    "    vocab_size=len(vocab),\n",
    "    seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = np.array(targets)\n",
    "contexts = np.array(contexts)[:,:,0]\n",
    "labels = np.array(labels)\n",
    "\n",
    "print('\\n')\n",
    "print(f\"targets.shape: {targets.shape}\")\n",
    "print(f\"contexts.shape: {contexts.shape}\")\n",
    "print(f\"labels.shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1024\n",
    "BUFFER_SIZE = 10000\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.target_embedding = layers.Embedding(vocab_size,\n",
    "                                          embedding_dim,\n",
    "                                          input_length=1,\n",
    "                                          name=\"w2v_embedding\")\n",
    "        self.context_embedding = layers.Embedding(vocab_size,\n",
    "                                           embedding_dim,\n",
    "                                           input_length=num_ns+1)\n",
    "\n",
    "    def call(self, pair):\n",
    "        target, context = pair\n",
    "        if len(target.shape) == 2:\n",
    "            target = tf.squeeze(target, axis=1)\n",
    "        # target: (batch,)\n",
    "        word_emb = self.target_embedding(target)\n",
    "        # word_emb: (batch, embed)\n",
    "        context_emb = self.context_embedding(context)\n",
    "        # context_emb: (batch, context, embed)\n",
    "        dots = tf.einsum('be,bce->bc', word_emb, context_emb)\n",
    "        # dots: (batch, context)\n",
    "        return dots"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
